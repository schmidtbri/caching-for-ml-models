{
 "cells": [
  {
   "cell_type": "raw",
   "id": "97711a24",
   "metadata": {},
   "source": [
    "Title: Caching for ML Model Deployments\n",
    "Date: 2022-08-10 07:00\n",
    "Category: Blog\n",
    "Slug: caching-for-ml-models\n",
    "Authors: Brian Schmidt\n",
    "Summary: In a software system, a [cache](https://en.wikipedia.org/wiki/Cache_(computing)) is a data store that is used to temporarily store computation results or frequently-accessed data. When accessing the results of a computation from a cache, we are able to avoid paying the cost of recomputing the result. When accessing a frequently accessed piece of data we are able to avoid paying the cost of accessing the data from a slower data store. This type of caching is used when accessing data from a slower data store than the cache. When a cache hit occurs, the data being sought is found and returned to the caller. When a “miss” occurs, the data is not found and must be recomputed or accessed from the slower data store by the caller. A data cache is generally built using storage that has low latency, which means that it is more expensive to run. Machine learning model deployments can benefit from caching because making predictions with a model can be a CPU-intensive process, especially for large and complex models. Predictions that take a long time to make can be cached and returned later when the same prediction is requested. This type of caching is also known as [memoization](https://en.wikipedia.org/wiki/Memoization). Another reason that a prediction can take a long time to create is if data enrichment is needed. Data enrichment is the process of adding fields to a model's input from a data store before a prediction is made, this process can add latency to the prediction and can benefit from caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca330c8",
   "metadata": {},
   "source": [
    "# Caching for ML Model Deployments\n",
    "\n",
    "In a [previous blog post](https://www.tekhnoal.com/ml-model-decorators.html) we introduced the decorator pattern for ML model deployments and then showed how to use the pattern to build extensions to a normal model deployment. For example, in [this blog post](https://www.tekhnoal.com/data-enrichment-for-ml-models.html) we added data enrichment to a deployed model. This extension was added without having to modify the machine learning model code at all, we were able to do it by using the decorator pattern. In this blog post we’ll add caching functionality to a model in the same way.\n",
    "\n",
    "This blog post was written in a Jupyter notebook, some of the code and commands found in it reflects this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44522dc6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In a software system, a [cache](https://en.wikipedia.org/wiki/Cache_(computing)) is a data store that is used to temporarily store computation results or frequently-accessed data. When accessing the results of a computation from a cache, we are able to avoid paying the cost of recomputing the result. When accessing a frequently accessed piece of data we are able to avoid paying the cost of accessing the data from a slower data store. This type of caching is used when accessing data from a slower data store than the cache. When a cache hit occurs, the data being sought is found and returned to the caller. When a “miss” occurs, the data is not found and must be recomputed or accessed from the slower data store by the caller. A data cache is generally built using storage that has low latency, which means that it is more expensive to run. \n",
    "\n",
    "Machine learning model deployments can benefit from caching because making predictions with a model can be a CPU-intensive process, especially for large and complex models. Predictions that take a long time to make can be cached and returned later when the same prediction is requested. This type of caching is also known as [memoization](https://en.wikipedia.org/wiki/Memoization). Another reason that a prediction can take a long time to create is if data enrichment is needed. Data enrichment is the process of adding fields to a model's input from a data store before a prediction is made, this process can add latency to the prediction and can benefit from caching.\n",
    "\n",
    "In order to enable prediction caching possible from ML models, we need to make sure that the model produces deterministic predictions. Determinism is a property of algorithms that says that the algorithm will always return the same output for the same input. If the model for which we want to cache predictions returns a different prediction for the same inputs, then we wouldn’t be able to cache the predictions at all since we wouldn’t be able to guarantee that the model would return the same prediction that we had cached.\n",
    "\n",
    "In this blog post, we’ll show how to create a simple decorator that is able to cache predictions for an ML model that is deployed to a production system. We'll also show how to deploy the decorator along with the model to a RESTful service.\n",
    "\n",
    "All of the code is available in this [github repository](https://github.com/schmidtbri/caching-for-ml-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d69be",
   "metadata": {},
   "source": [
    "## Software Architecture\n",
    "\n",
    "![Software Architecture](software_architecture_cfmlm.png)\n",
    "\n",
    "For caching predictions, we’ll be using [Redis](https://en.wikipedia.org/wiki/Redis). Redis is a data structure store that allows users to save and modify data structures in a remote service. This allows many clients to safely access the same data from a centralized service. Redis supports many different data structures, but we’ll be using the key-value store functionality to save our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdcd200",
   "metadata": {},
   "source": [
    "## Installing the Model\n",
    "\n",
    "To make this blog post a little shorter we won't train a completely new model. Instead we'll install a model that we've [built in a previous blog post](https://www.tekhnoal.com/regression-model.html). The code for the model is in [this github repository](https://github.com/schmidtbri/regression-model).\n",
    "\n",
    "To install the model, we can use the pip command and point it at the github repo of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f1b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "!pip install -e git+https://github.com/schmidtbri/regression-model#egg=insurance_charges_model\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f42fb",
   "metadata": {},
   "source": [
    "To make a prediction with the model, we'll import the model's class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58eb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurance_charges_model.prediction.model import InsuranceChargesModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516c054",
   "metadata": {},
   "source": [
    "Now we can instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84644224",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceChargesModel()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f4b41",
   "metadata": {},
   "source": [
    "To make a prediction, we'll need to use the model's input schema class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurance_charges_model.prediction.schemas import InsuranceChargesModelInput, \\\n",
    "    SexEnum, RegionEnum\n",
    "\n",
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5332f84",
   "metadata": {},
   "source": [
    "The model's input schema is called InsuranceChargesModelInput and it encompasses all of the features required by the model to make a prediction.\n",
    "\n",
    "Now we can make a prediction with the model by calling the predict() method with an instance of the InsuranceChargesModelInput class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b66f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30620b8",
   "metadata": {},
   "source": [
    "The model predicts that the charges will be $8640.78.\n",
    "\n",
    "We can view input schema of the model as a JSON schema document by calling the .schema() method on the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc015ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'InsuranceChargesModelInput',\n",
       " 'description': \"Schema for input of the model's predict method.\",\n",
       " 'type': 'object',\n",
       " 'properties': {'age': {'title': 'Age',\n",
       "   'description': 'Age of primary beneficiary in years.',\n",
       "   'minimum': 18,\n",
       "   'maximum': 65,\n",
       "   'type': 'integer'},\n",
       "  'sex': {'title': 'Sex',\n",
       "   'description': 'Gender of beneficiary.',\n",
       "   'allOf': [{'$ref': '#/definitions/SexEnum'}]},\n",
       "  'bmi': {'title': 'Body Mass Index',\n",
       "   'description': 'Body mass index of beneficiary.',\n",
       "   'minimum': 15.0,\n",
       "   'maximum': 50.0,\n",
       "   'type': 'number'},\n",
       "  'children': {'title': 'Children',\n",
       "   'description': 'Number of children covered by health insurance.',\n",
       "   'minimum': 0,\n",
       "   'maximum': 5,\n",
       "   'type': 'integer'},\n",
       "  'smoker': {'title': 'Smoker',\n",
       "   'description': 'Whether beneficiary is a smoker.',\n",
       "   'type': 'boolean'},\n",
       "  'region': {'title': 'Region',\n",
       "   'description': 'Region where beneficiary lives.',\n",
       "   'allOf': [{'$ref': '#/definitions/RegionEnum'}]}},\n",
       " 'definitions': {'SexEnum': {'title': 'SexEnum',\n",
       "   'description': \"Enumeration for the value of the 'sex' input of the model.\",\n",
       "   'enum': ['male', 'female'],\n",
       "   'type': 'string'},\n",
       "  'RegionEnum': {'title': 'RegionEnum',\n",
       "   'description': \"Enumeration for the value of the 'region' input of the model.\",\n",
       "   'enum': ['southwest', 'southeast', 'northwest', 'northeast'],\n",
       "   'type': 'string'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6e934",
   "metadata": {},
   "source": [
    "## Profiling the Model\n",
    "\n",
    "In order to get an idea of how much time it takes for our model to make a prediction, we'll profile it by making predictions with random data. To do this, we'll use the [Faker package](https://faker.readthedocs.io/en/master/). We can install it with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f9e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Faker\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be4bf7",
   "metadata": {},
   "source": [
    "We'll create a function that can generate a random sample that meets the model's input schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1fd8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "def generate_record() -> InsuranceChargesModelInput:\n",
    "    record = {\n",
    "        \"age\": faker.random_int(min=18, max=65),\n",
    "        \"sex\": faker.random_choices(elements=(\"male\", \"female\"), length=1)[0],\n",
    "        \"bmi\": faker.random_int(min=15000, max=50000)/1000.0,\n",
    "        \"children\": faker.random_int(min=0, max=5),\n",
    "        \"smoker\": faker.boolean(),\n",
    "        \"region\": faker.random_choices(elements=(\"southwest\", \"southeast\", \"northwest\", \"northeast\"), length=1)[0]\n",
    "    }\n",
    "    return InsuranceChargesModelInput(**record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774def2",
   "metadata": {},
   "source": [
    "The function returns an instance of the InsuranceChargesModelInput class, which is the type required by the model's predict() method. We'll use this function to profile the predict() method of the model.\n",
    "\n",
    "It's really hard to see a performance difference with one sample, so we'll perform a test with many random samples to see the difference. To start, we'll generate 1000 samples and save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c9f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    samples.append(generate_record())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92078c8f",
   "metadata": {},
   "source": [
    "By using the timeit module from the standard library, we can measure how much time it takes to call the model's predict method with a random sample. We'll make 1000 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0319ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "total_seconds = timeit.timeit(\"[model.predict(sample) for sample in samples]\", \n",
    "                              number=1, globals=globals())\n",
    "\n",
    "seconds_per_sample = total_seconds / len(samples)\n",
    "milliseconds_per_sample = seconds_per_sample * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323e67a2",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The model took 32.997 seconds to perform 1000 predictions, therefore it took 0.033 seconds to make a single prediction. \n",
       "The model takes about 32.997 milliseconds to make a prediction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The model took {} seconds to perform 1000 predictions, \"\n",
    "   \"therefore it took {} seconds to make a single prediction. \\n\"\n",
    "   \"The model takes about {} milliseconds to make a prediction.\".\n",
    "   format(round(total_seconds, 3), \n",
    "          round(seconds_per_sample, 4), \n",
    "          round(milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326d315",
   "metadata": {},
   "source": [
    "## Hashing Model Inputs\n",
    "\n",
    "Before we can build a caching decorator, we'll need to understand a little bit about hashing and how to use it for caching. A hashing operation is an operation takes in data of arbritrary size as input and returns data of a fixed size. A \"hash\" value refers to the fixed-size data that is returned from a hashing operation. Hashing has many uses in computer science, in this application we'll us hashing to uniquely identify some inputs that are provided to the ML model that we are decorating.\n",
    "\n",
    "Hashing is already built into the Python standard library through the hash() function, but it is only supported on certain types of objects. We can try it out using an instance of the model's input schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b709239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4360805119606244359"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92d0eb",
   "metadata": {},
   "source": [
    "To try out hashing, we converted an instance of the model's input schema into a dictionary, and then converted the keys and values of the dictionary into [frozensets](https://docs.python.org/3/library/stdtypes.html#frozenset). We then used the frozensets with the hash() function to create an integer value. The integer is the hashed value that we need to uniquely identify the inputs to the model.\n",
    "\n",
    "To see how hashing works, we'll create a separate input instance for the model that has the exact same values and hash it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5b66b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4360805119606244359"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62742c9",
   "metadata": {},
   "source": [
    "The hashed values are exactly the same, as we expected. The hashes value should be different if any of the values in the model input change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf041687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7065881474845529459"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.2,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095d0c7",
   "metadata": {},
   "source": [
    "The \"bmi\" field changed from 24.0 to 24.2, so we got a completely different hashed value.\n",
    "\n",
    "Hashing is a quick and easy way to identify inputs which will allow us to store the predictions of the model in the cache and retrieve them later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f958fb7",
   "metadata": {},
   "source": [
    "## Creating the Redis Cache Decorator\n",
    "\n",
    "We'll be using Redis to hold the cached predictions of the model. To access the Redis instance, we'll use the redis python package, which we'll install with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c01f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install redis\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b728e",
   "metadata": {},
   "source": [
    "Now we can implement the decorator class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "574152ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from ml_base.decorator import MLModelDecorator\n",
    "import redis\n",
    "import json\n",
    "\n",
    "\n",
    "class RedisCachingDecorator(MLModelDecorator):\n",
    "    \"\"\"Decorator for caching around an MLModel instance.\"\"\"\n",
    "\n",
    "    def __init__(self, host: str, port: int, database: int, prefix: Optional[str] = None, \n",
    "                 hashing_fields: Optional[List[str]] = None) -> None:\n",
    "        \n",
    "        super().__init__(host=host, port=port, database=database, prefix=prefix, \n",
    "                         hashing_fields=hashing_fields)\n",
    "        \n",
    "        self.__dict__[\"_redis_client\"] = redis.Redis(host=host, port=port, db=database)\n",
    "\n",
    "    def predict(self, data):\n",
    "        if self._configuration[\"prefix\"] is not None:\n",
    "            prefix = \"{}/{}/{}/\".format(self._configuration[\"prefix\"], \n",
    "                                        self._model.qualified_name, \n",
    "                                        self._model.version)\n",
    "        else:\n",
    "            prefix = \"{}/{}/\".format(self._model.qualified_name, \n",
    "                                     self._model.version)\n",
    "\n",
    "        # select hashing fields from input\n",
    "        if self._configuration[\"hashing_fields\"] is not None:\n",
    "            data_dict = {key: data.dict()[key] for key in self._configuration[\"hashing_fields\"]}\n",
    "        else:\n",
    "            data_dict = data.dict()\n",
    "        \n",
    "        # creating a key for the prediction inputs provided\n",
    "        frozen_data = frozenset(data_dict.keys()), frozenset(data_dict.values())\n",
    "        key = prefix + str(hash(frozen_data))\n",
    "       \n",
    "        # check if the prediction is in the cache\n",
    "        prediction = self.__dict__[\"_redis_client\"].get(key)\n",
    "        \n",
    "        # if the prediction is present in the cache, then deserialize it and return the prediction\n",
    "        if prediction is not None:\n",
    "            prediction = json.loads(prediction)\n",
    "            prediction = self._model.output_schema(**prediction)\n",
    "            return prediction\n",
    "        # if the prediction is not present in the cache, then make a prediction, save it to the cache, and return the prediction\n",
    "        else:\n",
    "            prediction = self._model.predict(data)\n",
    "            serialized_prediction = json.dumps(prediction.dict())\n",
    "            self.__dict__[\"_redis_client\"].set(key, serialized_prediction)\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffdd07",
   "metadata": {},
   "source": [
    "The caching decorator works very simply, when it receives inputs for the model it:\n",
    "\n",
    "- creates a key for the model input using hashing\n",
    "- checks if the key is present in the cache\n",
    "- if the key is present:\n",
    "    - retrieves the prediction for that key \n",
    "    - deserializes the contents of the cache into the output type of the model\n",
    "    - returns the prediction to the caller\n",
    "- if the key is not present:\n",
    "    - makes a prediction with the model it is decorating\n",
    "    - serializes the prediction to a JSON string\n",
    "    - saves the prediction to the cache with the key created\n",
    "    - returns the prediction to the caller\n",
    "\n",
    "The key created for each cache entry is made up of the model's qualified name, the model version and an optional prefix. The prefix is used to differentiate the predictions that are cached in a more flexible way. The caching decorator uses JSON as a serialization format to store information in the cache. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc173903",
   "metadata": {},
   "source": [
    "## Using the Redis Cache Decorator\n",
    "\n",
    "In order to try out the decorator, we'll need to run a local Redis instance. We can start one using Docker with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3403e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836c0d557926df641a2e657bcf0d935ec7b1e361b4de5dab6a9abad9371262ea\r\n"
     ]
    }
   ],
   "source": [
    "!docker run -d -p 6379:6379 --name local-redis redis/redis-stack-server:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b645fb8",
   "metadata": {},
   "source": [
    "To test out the decorator we first need to instantiate the model object that we want to use with the decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67724bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceChargesModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846001e",
   "metadata": {},
   "source": [
    "Next, we’ll instantiate the decorator with the connection parameters for the Redis docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8a71a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                          port=6379,\n",
    "                                          database=0,\n",
    "                                          prefix=\"prefix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f722a9",
   "metadata": {},
   "source": [
    "We can add the model instance to the decorator after it’s been instantiated like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec74d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_model = caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58efd0",
   "metadata": {},
   "source": [
    "We can see the decorator and the model objects by printing the reference to the decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d36281f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RedisCachingDecorator(InsuranceChargesModel)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decorated_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305be27c",
   "metadata": {},
   "source": [
    "The decorator object is printing out it's own type along with the type of the model that it is decorating.\n",
    "\n",
    "Now we’ll try to use the decorator and the model together by making a few predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1455952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=9612.64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=46,\n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03220352",
   "metadata": {},
   "source": [
    "The first time we make a prediction with a given input, we'll get the prediction made by the model and the decorator will store the prediction in the cache. \n",
    "\n",
    "We can view the key in the redis database to see how it is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "529cd329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "prefix/insurance_charges_model/0.1.0/5926980192354242260\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli SCAN 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bee5f1",
   "metadata": {},
   "source": [
    "There is a single key in the redis database. We'll access they contents of the key like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7674a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"charges\": 9612.64}\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli GET prefix/insurance_charges_model/0.1.0/5926980192354242260"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4834f3",
   "metadata": {},
   "source": [
    "The prediction is stored in the key as a JSON string.\n",
    "\n",
    "We'll try the same prediction again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea103734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=9612.64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=46, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcdb8c",
   "metadata": {},
   "source": [
    "This time the prediction was not made by the model, it was found in the Redis cache and returned by the decorator instead of being made again.\n",
    "\n",
    "Next, we'll use the 1000 samples we generated above to make predictions with the decorated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6224b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_total_seconds = timeit.timeit(\"[decorated_model.predict(sample) for sample in samples]\", \n",
    "                                        number=1, globals=globals())\n",
    "\n",
    "decorated_seconds_per_sample = decorated_total_seconds / len(samples)\n",
    "decorated_milliseconds_per_sample = decorated_seconds_per_sample * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9695ff5f",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorated model took 36.419 seconds to perform 1000 predictions the first time that it saw the prediction inputs, therefore it took 0.0364 seconds to make a single prediction. \n",
       "The decorated model takes about 36.419 milliseconds to make a prediction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The decorated model took {} seconds to perform 1000 predictions the first time that it saw \"\n",
    "   \"the prediction inputs, therefore it took {} seconds to make a single prediction. \"\n",
    "   \"\\nThe decorated model takes about {} milliseconds to make a prediction.\".\n",
    "   format(round(decorated_total_seconds, 3), \n",
    "          round(decorated_seconds_per_sample, 4), \n",
    "          round(decorated_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf40f52",
   "metadata": {},
   "source": [
    "We'll run the same samples through again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d48f335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_total_seconds = timeit.timeit(\"[decorated_model.predict(sample) for sample in samples]\", \n",
    "                                        number=1, globals=globals())\n",
    "\n",
    "decorated_seconds_per_sample = decorated_total_seconds / len(samples)\n",
    "decorated_milliseconds_per_sample = decorated_seconds_per_sample * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c748998",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorated model took 0.88 seconds to perform 1000 predictions the second time that it saw the prediction inputs, therefore it took 0.0009 seconds to make a single prediction. \n",
       "The decorated model takes about 0.88 milliseconds to access a single prediction and return it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The decorated model took {} seconds to perform 1000 predictions the second time that it saw \"\n",
    "   \"the prediction inputs, therefore it took {} seconds to make a single prediction. \"\n",
    "   \"\\nThe decorated model takes about {} milliseconds to access a single prediction and return it.\".\n",
    "   format(round(decorated_total_seconds, 3), \n",
    "          round(decorated_seconds_per_sample, 4), \n",
    "          round(decorated_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287da0d",
   "metadata": {},
   "source": [
    "It took less time because the cached predictions were returned more quickly because we requested the same predictions from the model.\n",
    "\n",
    "We can get the amount of memory used by the cache by accessing the keys and summing up the number of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51184547",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "\n",
    "decorated_number_of_bytes = 0\n",
    "decorated_total_entries = 0\n",
    "for key in r.scan_iter(\"prefix*\"):\n",
    "    decorated_number_of_bytes += len(r.get(key))\n",
    "    decorated_total_entries = decorated_total_entries + 1\n",
    "    \n",
    "decorated_average_number_of_bytes = decorated_number_of_bytes / decorated_total_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bf0adca",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The keys in the cache take up a total of 20624 bytes. The average number of bytes per cache entry is 20.6."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The keys in the cache take up a total of {} bytes. \"\n",
    "   \"The average number of bytes per cache entry is {}.\"\n",
    "   .format(decorated_number_of_bytes, \n",
    "           round(decorated_average_number_of_bytes, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa033e5",
   "metadata": {},
   "source": [
    "We'll clear the redis database to make sure the contents don't intefere with the next things we want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e47aaf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b61cb7",
   "metadata": {},
   "source": [
    "## Selecting Fields For Hashing\n",
    "\n",
    "In certain situations, not all of the fields in the model's input should be used to create a hash. This may be because not all of the model's input fields are actually used for making a prediction. Some fields may be used for logging or debugging and do not actually affect the prediction created by the model. If changing the value of a field does not affect the value of the prediction created by the model, it should not be used to create the hashed key for the cache.\n",
    "\n",
    "The caching decorator supports selecting specific fields from the input to create the cache key. The option is called \"hashing_fields\" and is provided to the decorator instance like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7466fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                          port=6379,\n",
    "                                          database=0,\n",
    "                                          prefix=\"prefix\",\n",
    "                                          hashing_fields=[\"age\", \"sex\", \"bmi\", \"children\", \"smoker\"])\n",
    "\n",
    "decorated_model = caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681df7bf",
   "metadata": {},
   "source": [
    "The decorator now uses all of the input fields except for the \"region\" field to create the key.\n",
    "\n",
    "To try out the functionality, we'll create a prediction with the decorated model. The prediction will get saved in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16f00b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15219.19)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=52, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=3,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630c48d",
   "metadata": {},
   "source": [
    "We'll now make the same prediction, but this time the prediction will come from the cache because it was saved there previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f891b11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15219.19)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=52, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=3,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516dcf9",
   "metadata": {},
   "source": [
    "We'll make the prediction one more time, but this time we'll change the value of the \"region\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6892a4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15219.19)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=52, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=3,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.southeast)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ec9ca",
   "metadata": {},
   "source": [
    "The predicted value should have changed because the region changed. It didn't change because we accessed the prediction from the cache instead of creating a new one. This happened because we ignored the value of the \"region\" field when creating the hashed key in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9173c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9aacfe",
   "metadata": {},
   "source": [
    "## Improving the Performance of the Decorator\n",
    "\n",
    "When a prediction is stored in the cache, it is currently serialized using the JSON format. This format is simple and easy to understand, but it is not the most efficient format for serialization in terms of the size of the data and the time it takes to do the serialization.\n",
    "\n",
    "To try to improve the efficiency of the caching decorator we'll add options for other serialization formats and also try to use compression. Another way to reduce the memory usage of the cache is to reduce the precision of the numbers given to the model. These approaches will be fully explained below.\n",
    "\n",
    "We'll be using [MessagePack](https://msgpack.org/index.html) to do serialization and [Snappy](https://en.wikipedia.org/wiki/Snappy_(compression)) for compression, so we need to install the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09e1b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install msgpack\n",
    "!pip install python-snappy\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed523cce",
   "metadata": {},
   "source": [
    "We'll recreate the RedisCachingDecorator class with the code needed to support the new features we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78e1e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "import snappy\n",
    "\n",
    "\n",
    "class RedisCachingDecorator(MLModelDecorator):\n",
    "    \"\"\"Decorator for caching around an MLModel instance.\"\"\"\n",
    "\n",
    "    def __init__(self, host: str, port: int, database: int, prefix: Optional[str] = None, \n",
    "                 hashing_fields: Optional[List[str]] = None, serder: str = \"JSON\", \n",
    "                 use_compression: bool = False, \n",
    "                 reduced_precision: bool = False,\n",
    "                 number_of_places: Optional[int] = None\n",
    "                ) -> None:\n",
    "        \n",
    "        if serder not in [\"JSON\", \"MessagePack\"]:\n",
    "            raise ValueError(\"Serder option not supported.\")\n",
    "            \n",
    "        if reduced_precision is True and number_of_places is None:\n",
    "            raise ValueError(\"number_of_places must be provided when reduced_precision is True.\")\n",
    "            \n",
    "        if number_of_places is None and reduced_precision is True:\n",
    "            raise ValueError(\"reduced_precision must be True when number_of_places is provided.\")\n",
    "        \n",
    "        super().__init__(host=host, port=port, database=database, prefix=prefix, \n",
    "                         hashing_fields=hashing_fields, serder=serder, \n",
    "                         use_compression=use_compression, \n",
    "                         reduced_precision=reduced_precision,\n",
    "                         number_of_places=number_of_places)\n",
    "        \n",
    "        self.__dict__[\"_redis_client\"] = redis.Redis(host=host, port=port, db=database)\n",
    "\n",
    "    def predict(self, data):\n",
    "        if self._configuration[\"prefix\"] is not None:\n",
    "            prefix = \"{}/{}/{}/\".format(self._configuration[\"prefix\"], \n",
    "                                        self._model.qualified_name, \n",
    "                                        self._model.version)\n",
    "        else:\n",
    "            prefix = \"{}/{}/\".format(self._model.qualified_name,\n",
    "                                     self._model.version)\n",
    "        \n",
    "        # reducing the precision of the numerical fields, if it is enabled\n",
    "        if self._configuration[\"reduced_precision\"] is True:\n",
    "                for field_name, field_attributes in self._model.input_schema.schema()[\"properties\"].items():\n",
    "                    if \"type\" in field_attributes.keys() and field_attributes[\"type\"] == \"number\":\n",
    "                        field_value = getattr(data, field_name)\n",
    "                        setattr(data, field_name, round(field_value, self._configuration[\"number_of_places\"]))\n",
    "\n",
    "        # select hashing fields from input\n",
    "        if self._configuration[\"hashing_fields\"] is not None:\n",
    "            data_dict = {key: data.dict()[key] for key in self._configuration[\"hashing_fields\"]}\n",
    "        else:\n",
    "            data_dict = data.dict()\n",
    "        \n",
    "        # creating a key for the prediction inputs provided\n",
    "        frozen_data = frozenset(data_dict.keys()), frozenset(data_dict.values())\n",
    "        key = prefix + str(hash(frozen_data))\n",
    "       \n",
    "        # check if the prediction is in the cache\n",
    "        prediction = self.__dict__[\"_redis_client\"].get(key)\n",
    "        \n",
    "        # if the prediction is present in the cache\n",
    "        if prediction is not None:\n",
    "\n",
    "            # optionally decompressing the bytes\n",
    "            if self._configuration[\"use_compression\"]:\n",
    "                decompressed_prediction = snappy.decompress(prediction)\n",
    "            else:\n",
    "                decompressed_prediction = prediction\n",
    "            \n",
    "            # deserializing to bytes\n",
    "            if self._configuration[\"serder\"] == \"JSON\":\n",
    "                deserialized_prediction = json.loads(decompressed_prediction.decode())\n",
    "            elif self._configuration[\"serder\"] == \"MessagePack\":\n",
    "                deserialized_prediction = msgpack.loads(decompressed_prediction)\n",
    "            else: \n",
    "                raise ValueError(\"Serder option not supported.\")\n",
    "                \n",
    "            # creating the output instance\n",
    "            prediction = self._model.output_schema(**deserialized_prediction)\n",
    "\n",
    "            return prediction\n",
    "\n",
    "        # if the prediction is not present in the cache\n",
    "        else:\n",
    "            # making a prediction with the model\n",
    "            prediction = self._model.predict(data)\n",
    "\n",
    "            # serializing to bytes\n",
    "            if self._configuration[\"serder\"] == \"JSON\":\n",
    "                serialized_prediction = str.encode(json.dumps(prediction.dict()))\n",
    "            elif self._configuration[\"serder\"] == \"MessagePack\":\n",
    "                serialized_prediction = msgpack.dumps(prediction.dict())\n",
    "            else: \n",
    "                raise ValueError(\"Serder option not supported.\")\n",
    "                \n",
    "            # optionally compressing the bytes\n",
    "            if self._configuration[\"use_compression\"]:\n",
    "                serialized_prediction = snappy.compress(serialized_prediction)\n",
    "                \n",
    "            # saving the prediction to the cache\n",
    "            self.__dict__[\"_redis_client\"].set(key, serialized_prediction)\n",
    "\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32f733",
   "metadata": {},
   "source": [
    "The new implementation above includes options to enable MessagePack for serialization/deserialization, snappy for compression, and the ability to reduce the precision of numerical fields in the model input. We'll try out each option individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f655bf3",
   "metadata": {},
   "source": [
    "### MessagePack Serialization\n",
    "\n",
    "[MessagePack](https://msgpack.org/index.html) is a binary serialization format designed for small, efficient and flexible serialization. \n",
    "\n",
    "To enable MessagePack, we'll instantiate the decorator setting the \"serder\" option to \"MessagePack\". We'll use a prefix to separate the cache entries that use MessagePack from the other cache entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc3bee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpack_caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                                  port=6379,\n",
    "                                                  database=0,\n",
    "                                                  prefix=\"msgpack\",\n",
    "                                                  serder=\"MessagePack\")\n",
    "\n",
    "mspgpack_decorated_model = msgpack_caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712fe66",
   "metadata": {},
   "source": [
    "The first time we make a prediction, the model will be used and the prediction will get serialized to MessagePack and saved to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddd14e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15113.29)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=55, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=25.0,\n",
    "    children=4,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = mspgpack_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104bde4",
   "metadata": {},
   "source": [
    "The second time we make a prediction, the cache entry will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e91aed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15113.29)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=55, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=25.0,\n",
    "    children=4,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = mspgpack_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaeed6",
   "metadata": {},
   "source": [
    "The MessagePack format works, now we'll do some testing to see if it improves the serialization/deserialization performance.\n",
    "\n",
    "As before, we'll make the predictions on the samples to fill in the cache with predictions. We'll be using the 1000 samples generated above to keep the comparison fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "456cc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpack_total_seconds = timeit.timeit(\"[mspgpack_decorated_model.predict(sample) for sample in samples]\", \n",
    "                                      number=1, globals=globals())\n",
    "\n",
    "msgpack_seconds_per_sample = msgpack_total_seconds / len(samples)\n",
    "msgpack_milliseconds_per_sample = msgpack_seconds_per_sample * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83f83e24",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorated model that uses MessagePack took 35.627 seconds to perform 1000 predictions the first time that it saw the prediction inputs. The decorated model takes about 35.627 milliseconds to make a single prediction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The decorated model that uses MessagePack took {} seconds to perform 1000 predictions the first \"\n",
    "   \"time that it saw the prediction inputs. The decorated model takes about {} milliseconds to make a single \"\n",
    "   \"prediction.\".format(round(msgpack_total_seconds, 3), \n",
    "                        round(msgpack_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2fb42",
   "metadata": {},
   "source": [
    "Most of the time for this step is taken up by the model's prediction algorithm, this is the reason why its a similar amount of time as the JSON serder we used before.\n",
    "\n",
    "Now we can try the same predictions again. This time, they'll be accessed from the cache and returned more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1839a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpack_total_seconds = timeit.timeit(\"[mspgpack_decorated_model.predict(sample) for sample in samples]\", \n",
    "                                      number=1, globals=globals())\n",
    "\n",
    "msgpack_seconds_per_sample = msgpack_total_seconds / len(samples)\n",
    "msgpack_milliseconds_per_sample = msgpack_seconds_per_sample * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3de1f271",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The model that uses MessagePack took 0.955 seconds to perform 1000 predictions the second time that it saw the prediction inputs. The decorated model takes about 0.955 milliseconds to access a single prediction and return it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The model that uses MessagePack took {} seconds to perform 1000 predictions the second time that it saw \"\n",
    "   \"the prediction inputs. The decorated model takes about {} milliseconds to access a single prediction and \"\n",
    "   \"return it.\".format(round(msgpack_total_seconds, 3),  \n",
    "                       round(msgpack_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20b5f5c1",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The MessagePack serder performs at around the same speed as the JSON serder. The test we did with JSON above took about 0.88 ms for each sample, the MessagePack serder took 0.955 ms per sample."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The MessagePack serder performs at around the same speed as the JSON serder. \"\n",
    "   \"The test we did with JSON above took about {} ms for each sample, \"\n",
    "   \"the MessagePack serder took {} ms per sample.\"\n",
    "   .format(round(decorated_milliseconds_per_sample, 3), \n",
    "           round(msgpack_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd09f8",
   "metadata": {},
   "source": [
    "We can see how much space the cache entries is taking up by querying each key and summing up the number of bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3cd2d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpack_number_of_bytes = 0\n",
    "msgpack_total_entries = 0\n",
    "for key in r.scan_iter(\"msgpack*\"):\n",
    "    msgpack_number_of_bytes += len(r.get(key))\n",
    "    msgpack_total_entries = msgpack_total_entries + 1\n",
    "    \n",
    "msgpack_average_number_of_bytes = msgpack_number_of_bytes / msgpack_total_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfe522f4",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The keys in the original JSON cache took up a total of 20624 bytes. The keys in the MessagePack cache take up a total of 18018 bytes and the average number of bytes per MessagePack cache entry is 18.0."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The keys in the original JSON cache took up a total of {} bytes. \"\n",
    "   \"The keys in the MessagePack cache take up a total of {} bytes and the \"\n",
    "   \"average number of bytes per MessagePack cache entry is {}.\"\n",
    "   .format(decorated_number_of_bytes,\n",
    "           msgpack_number_of_bytes, \n",
    "           round(msgpack_average_number_of_bytes,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e44f5",
   "metadata": {},
   "source": [
    "By using MessagePack serialization we were able to use less memory in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f274c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6c860",
   "metadata": {},
   "source": [
    "### Snappy Compression\n",
    "\n",
    "[Snappy](https://github.com/google/snappy) is a compression algorithm built by Google that targets high compression ratios and high compressions speed. We can try to reduce the memory used by the cache by compressing the cache entries with the Snappy algorithm. This approach was inspired by [another blog post](https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/).\n",
    "\n",
    "Enabling compression on the decorator is very simple, we'll just set the \"use_compression\" parameter to \"True\" when instantiating the caching decorator. In this example we'll use JSON serialization combined with compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d3e9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressing_caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                                      port=6379,\n",
    "                                                      database=0,\n",
    "                                                      prefix=\"json+compression\",\n",
    "                                                      serder=\"JSON\",\n",
    "                                                      use_compression=True)\n",
    "\n",
    "compressing_decorated_model = compressing_caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3125f1",
   "metadata": {},
   "source": [
    "The first time we make a prediction, the model will be used and the prediction will get serialized to JSON, then compressed, and saved to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbf88b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15207.01)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=53, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=25.0,\n",
    "    children=4,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = compressing_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fe967",
   "metadata": {},
   "source": [
    "The second time we make a prediction, the compressed cache entry will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c0c2b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15207.01)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=53, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=25.0,\n",
    "    children=4,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = compressing_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654692c",
   "metadata": {},
   "source": [
    "The compression works, now we'll do some testing to see if it improves the serialization/deserialization performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3def91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_total_seconds = timeit.timeit(\"[compressing_decorated_model.predict(sample) for sample in samples]\", \n",
    "                                         number=1, globals=globals())\n",
    "\n",
    "compressed_seconds_per_sample = compressed_total_seconds / len(samples)\n",
    "compressed_milliseconds_per_sample = compressed_seconds_per_sample * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e7e9431",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorator that does compression took around 35.224 ms to make a prediction and add it to the cache the first time that it sees the prediction inputs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The decorator that does compression took around {} ms to make a prediction and add it to the cache the \"\n",
    "   \"first time that it sees the prediction inputs.\".\n",
    "   format(round(compressed_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0b98c",
   "metadata": {},
   "source": [
    "Most of the time for this step is taken up by the model's prediction algorithm.\n",
    "\n",
    "Now we can try the same predictions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1504d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_total_seconds = timeit.timeit(\"[compressing_decorated_model.predict(sample) for sample in samples]\", \n",
    "                                         number=1, globals=globals())\n",
    "\n",
    "compressed_seconds_per_sample = compressed_total_seconds / len(samples)\n",
    "compressed_milliseconds_per_sample = compressed_seconds_per_sample * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "277a16d4",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorator that uses compressed JSON took 0.906 ms to make a prediction the second time that it saw the prediction inputs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The decorator that uses compressed JSON took {} ms to make a prediction the second time that it saw\"\n",
    "   \" the prediction inputs.\".format(round(compressed_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68c1977c",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The serder that uses JSON serialization and compression performs around the same as the JSON serder. The test we did with uncompressed JSON above took about 0.88 ms for each sample."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The serder that uses JSON serialization and compression performs around the same as the JSON serder. \"\n",
    "   \"The test we did with uncompressed JSON above took about {} ms for each sample.\".\n",
    "   format(round(decorated_milliseconds_per_sample,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8043f1",
   "metadata": {},
   "source": [
    "We can see how much space the cache entries is taking up by querying each key and summing up the number of bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df953c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_number_of_bytes = 0\n",
    "compressed_total_entries = 0\n",
    "for key in r.scan_iter(\"json+compression*\"):\n",
    "    compressed_number_of_bytes += len(r.get(key))\n",
    "    compressed_total_entries = compressed_total_entries + 1\n",
    "    \n",
    "compressed_average_number_of_bytes = compressed_number_of_bytes / compressed_total_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be3093fb",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The keys in the original JSON cache took up a total of 20624 bytes. The keys in the MessagePack cache take up a total of 18018 bytes. The keys in the compressed JSON cache take up a total of 22627 bytes, and the average number of bytes per cache entry is 22.6."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The keys in the original JSON cache took up a total of {} bytes. \"\n",
    "   \"The keys in the MessagePack cache take up a total of {} bytes. \"\n",
    "   \"The keys in the compressed JSON cache take up a total of {} bytes, \"\n",
    "   \"and the average number of bytes per cache entry is {}.\".\n",
    "   format(decorated_number_of_bytes, \n",
    "          msgpack_number_of_bytes,\n",
    "          compressed_number_of_bytes,\n",
    "          round(compressed_average_number_of_bytes, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0e8f2",
   "metadata": {},
   "source": [
    "The keys that were serialized with JSON and compressed were a few bytes bigger than the keys serialized and not compressed. It seems that compression is not saving memory in the cache, this is probably due to the small size of the entries and the fact that information was not repeated inside of the serialized data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8182e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492548d",
   "metadata": {},
   "source": [
    "### Reducing the Precision of the Inputs\n",
    "\n",
    "We can also try to limit the size of the cache by reducing the number of possible inputs to the hashing function. We'll demonstrate this with a few examples.\n",
    "\n",
    "We'll start by hashing a single sample of the input of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8dc6e661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2801283067008197552"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12345,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe843a7",
   "metadata": {},
   "source": [
    "Next, we'll hash a very similar model input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5b01610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5034586836711654789"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12346,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21120ce",
   "metadata": {},
   "source": [
    "The hash value produced is the second time is completely different even though the \"bmi\" field only changed by 0.00001. This means that these two predictions will have two different cache entries even though they are very lilely to produce exactly the same prediction. Just to make sure, we'll make the predictions using these inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bba9906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12345,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdc911",
   "metadata": {},
   "source": [
    "Let's try the prediction and hash with a different value for the \"bmi\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85d460e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12346,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2c797",
   "metadata": {},
   "source": [
    "The prediction came out to be the same for both values of \"bmi\". However, the hashed value of the input was completely different. These predictions would be saved separately from each other in the cache, event though they are exactly the same. We can cut down on the number of entries in the cache by reducing the precision of floating point numbers so that these predictions can be cached one time instead of many. By rounding down the number we'll be reducing the number of cache entries that will be placed in the cache but also affecting the accuracy of the model's predictions. \n",
    "\n",
    "The caching decorator supports this feature, we'll just enable it by adding the \"reduced_precision\" and \"number_of_places\" options to the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65e965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_precision_caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                                        port=6379,\n",
    "                                                        database=0,\n",
    "                                                        prefix=\"low_precision\",\n",
    "                                                        reduced_precision=True,\n",
    "                                                        number_of_places=0)\n",
    "\n",
    "low_precision_decorated_model = low_precision_caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625ccd9",
   "metadata": {},
   "source": [
    "The first time we make a prediction, the model will be used and the prediction input will get the precision of the \"bmi\" field reduced to one decimal place, then the prediction will get serialized to JSON, and saved to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d13d1625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12345,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = low_precision_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e507afa",
   "metadata": {},
   "source": [
    "The second time the prediction is requested, the precision of the \"bmi\" field is reduced again in the same way, making the prediction input the same as before even though the values for the \"bmi\" field are not exactly the same. This will create the same hashed value which will retrieve the prediction from the cache and return it to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be4d0f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.4321,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = low_precision_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd6560",
   "metadata": {},
   "source": [
    "The predictions are the same even though the inputs were different. We can view the keys in the cache like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdbb0a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "low_precision/insurance_charges_model/0.1.0/-4360805119606244359\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli SCAN 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99d4b8",
   "metadata": {},
   "source": [
    "There's only one entry in the cache, which means that first prediction was used and no new entry was made for the second set of inputs.\n",
    "\n",
    "Although this is not always an ideal way to save memory, there are some model deployments that can benefit from this approach. All that is needed is to analyze how much precision the model needs from its numerical inputs. It rarely makes sense to store predictions with an unlimited precision in their numerical inputs in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7e1c0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c9ff8",
   "metadata": {},
   "source": [
    "## Adding the Decorator to a Deployed Model\n",
    "\n",
    "Now that we have a working decorator, we can use it inside of a service alongside the model. To do this, we'll use the [rest_model_service](https://pypi.org/project/rest-model-service/) package to quickly create a RESTful service. You can learn more about this package in [this blog post](https://www.tekhnoal.com/rest-model-service.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97239224",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rest_model_service\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17f26e",
   "metadata": {},
   "source": [
    "To create a service for our model, all that is needed is that we add a YAML configuration file to the project. The configuration file looks like this:\n",
    "\n",
    "```yaml\n",
    "service_title: Insurance Charges Model Service\n",
    "models:\n",
    "  - qualified_name: insurance_charges_model\n",
    "    class_path: insurance_charges_model.prediction.model.InsuranceChargesModel\n",
    "    create_endpoint: true\n",
    "    decorators:\n",
    "      - class_path: ml_model_caching.redis.RedisCachingDecorator\n",
    "        configuration:\n",
    "          host: \"localhost\"\n",
    "          port: 6379\n",
    "          database: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605046aa",
   "metadata": {},
   "source": [
    "The service_title field is the name of the service as it will appear in the documentation. The models field is an array that contains the details of the models we would like to deploy in the service. The class_path points at the MLModel class that implement's the model's prediction logic, in this case we'll be using the same model as in the examples above. The decorators field contains the details of the decorators that we want to attach to the model instance. We want to use the RedisCachingDecorator decorator class with the configuration we've used for local testing.\n",
    "\n",
    "To run the service locally, execute these commands:\n",
    "\n",
    "```bash\n",
    "export PYTHONPATH=./\n",
    "export REST_CONFIG=./configuration/rest_configuration.yaml\n",
    "uvicorn rest_model_service.main:app --reload\n",
    "```\n",
    "\n",
    "The service should come up and can be accessed in a web browser at http://127.0.0.1:8000. When you access that URL using a web browser you will be redirected to the documentation page that is generated by the FastAPI package.\n",
    "\n",
    "We can try out the service with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "667f381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"charges\":46277.67}"
     ]
    }
   ],
   "source": [
    "!curl -X 'POST' \\\n",
    "  'http://127.0.0.1:8000/api/models/insurance_charges_model/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{ \\\n",
    "        \\\"age\\\": 65, \\\n",
    "        \\\"sex\\\": \\\"male\\\", \\\n",
    "        \\\"bmi\\\": 50, \\\n",
    "        \\\"children\\\": 5, \\\n",
    "        \\\"smoker\\\": true, \\\n",
    "        \\\"region\\\": \\\"southwest\\\" \\\n",
    "    }\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623d5a2",
   "metadata": {},
   "source": [
    "We can check the Redis instance to make sure that the cache is being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1dfebfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "insurance_charges_model/0.1.0/-3948524794153351987\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli SCAN 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2613d26",
   "metadata": {},
   "source": [
    "By using the MLModel base class provided by the ml_base package and the REST service framework provided by the rest_model_service package we're able to quickly stand up a service to host the model. The decorator that we want to test can also be added to the model through configuration, including all of its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2978e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1a57a",
   "metadata": {},
   "source": [
    "## Deploying the Caching Decorator\n",
    "\n",
    "Now that we have a working model and model service, we'll need to deploy it somewhere. We'll start by deploying the service locally using Docker. Once we have the service and Redis working locally, we'll deploy everything to a local Minikube instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165d5ce",
   "metadata": {},
   "source": [
    "### Creating a Docker Image\n",
    "\n",
    "Let's create a docker image and run it locally. The docker image is generated using instructions in the Dockerfile:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "ARG BUILD_DATE\n",
    "\n",
    "LABEL org.opencontainers.image.title=\"Caching for ML Models\"\n",
    "LABEL org.opencontainers.image.description=\"Caching for machine learning models.\"\n",
    "LABEL org.opencontainers.image.created=$BUILD_DATE\n",
    "LABEL org.opencontainers.image.authors=\"6666331+schmidtbri@users.noreply.github.com\"\n",
    "LABEL org.opencontainers.image.source=\"https://github.com/schmidtbri/caching-for-ml-models\"\n",
    "LABEL org.opencontainers.image.version=\"0.1.0\"\n",
    "LABEL org.opencontainers.image.licenses=\"MIT License\"\n",
    "LABEL org.opencontainers.image.base.name=\"python:3.9-slim\"\n",
    "\n",
    "WORKDIR ./service\n",
    "\n",
    "# installing git because we need to install the model package from the github repository\n",
    "RUN apt-get update\n",
    "RUN apt-get --assume-yes install git\n",
    "\n",
    "COPY ./ml_model_caching ./ml_model_caching\n",
    "COPY ./configuration ./configuration\n",
    "COPY ./LICENSE ./LICENSE\n",
    "COPY ./service_requirements.txt ./service_requirements.txt\n",
    "\n",
    "RUN pip install -r service_requirements.txt\n",
    "\n",
    "CMD [\"uvicorn\", \"rest_model_service.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16fdde",
   "metadata": {},
   "source": [
    "The Dockerfile is used by this command to create a docker image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dad97f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t insurance_charges_model_service:latest ../\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbda0ad",
   "metadata": {},
   "source": [
    "To make sure everything worked as expected, we'll look through the docker images in our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "28984acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance_charges_model_service   latest    2c8c19151e65   32 hours ago   1.26GB\r\n"
     ]
    }
   ],
   "source": [
    "!docker image ls | grep insurance_charges_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed2912",
   "metadata": {},
   "source": [
    "Next, we'll start the image to see if everything is working as expected. To do this we'll create a local docker network and connect the redis container and the model service container to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "45c57543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1d8ad0b59ad831f1c6205cea3e799ee31f40109006b9a02d39db8207a7e3f339\r\n"
     ]
    }
   ],
   "source": [
    "!docker network create local-network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe9b8e",
   "metadata": {},
   "source": [
    "We'll connect the running redis container that we were working with to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ca2a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker network connect local-network local-redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be85c1",
   "metadata": {},
   "source": [
    "Now we can start the service docker image connected to the same network as the redis container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2bd16c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83db77417dfa5cd33c3d7fabea8349df8b3932ef0cd2544a94b7d4958eed93bc\r\n"
     ]
    }
   ],
   "source": [
    "!docker run -d \\\n",
    "    -p 8000:8000 \\\n",
    "    --net local-network \\\n",
    "    -e REST_CONFIG=./configuration/local_rest_config.yaml \\\n",
    "    --name insurance_charges_model_service \\\n",
    "    insurance_charges_model_service:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31288d",
   "metadata": {},
   "source": [
    "Notice that we're using a different configuration file that has a different hostname for the redis instance. The redis container is not accesible from localhost inside of the network so we needed to have the hostname \"local-redis\" in the configuration.\n",
    "\n",
    "The service should be accessible on port 8000 of localhost, so we'll try to make a prediction using the curl command running inside of a container connected to the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ebfb6916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"charges\":46277.67}"
     ]
    }
   ],
   "source": [
    "!docker run -it --rm \\\n",
    "    --net local-network \\\n",
    "    curlimages/curl \\\n",
    "    curl -X 'POST' \\\n",
    "    'http://insurance_charges_model_service:8000/api/models/insurance_charges_model/prediction' \\\n",
    "    -H 'accept: application/json' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d \"{ \\\n",
    "        \\\"age\\\": 65, \\\n",
    "        \\\"sex\\\": \\\"male\\\", \\\n",
    "        \\\"bmi\\\": 50, \\\n",
    "        \\\"children\\\": 5, \\\n",
    "        \\\"smoker\\\": true, \\\n",
    "        \\\"region\\\": \\\"southwest\\\" \\\n",
    "    }\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b40a76",
   "metadata": {},
   "source": [
    "The model predicted that the insurance charges would be $46277.67 and also saved the prediction to the Redis cache. We can view the cache entries in Redis with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fa2510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "insurance_charges_model/0.1.0/7732985413081947687\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli SCAN 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21dc7aa",
   "metadata": {},
   "source": [
    "The key in the cache has this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3dd7d9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"charges\": 46277.67}\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli GET insurance_charges_model/0.1.0/7732985413081947687"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d217584",
   "metadata": {},
   "source": [
    "Since we didn't use MessagePack or Snappy compression the value is easily read as a plain JSON string.\n",
    "\n",
    "Now that we're done with the local redis instance we'll stop and remove the docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39019a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-redis\n",
      "local-redis\n",
      "insurance_charges_model_service\n",
      "insurance_charges_model_service\n",
      "local-network\n"
     ]
    }
   ],
   "source": [
    "!docker kill local-redis\n",
    "!docker rm local-redis\n",
    "\n",
    "!docker kill insurance_charges_model_service\n",
    "!docker rm insurance_charges_model_service\n",
    "\n",
    "!docker network rm local-network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850cbbf4",
   "metadata": {},
   "source": [
    "## Deploying the Solution\n",
    "\n",
    "To show the system in action, we’ll deploy the service and the Redis instance to a Kubernetes cluster. A local cluster can be easily started by using [minikube](https://minikube.sigs.k8s.io/docs/). Installation instructions can be found [here](https://minikube.sigs.k8s.io/docs/start/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21249e0c",
   "metadata": {},
   "source": [
    "### Creating the Kubernetes Cluster\n",
    "\n",
    "To start the minikube cluster execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ac2c04ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄  minikube v1.26.1 on Darwin 12.5\n",
      "✨  Using the virtualbox driver based on existing profile\n",
      "👍  Starting control plane node minikube in cluster minikube\n",
      "🔄  Restarting existing virtualbox VM for \"minikube\" ...\n",
      "🐳  Preparing Kubernetes v1.24.3 on Docker 20.10.17 ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "🔎  Verifying Kubernetes components...\n",
      "    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n",
      "    ▪ Using image kubernetesui/dashboard:v2.6.0\n",
      "    ▪ Using image kubernetesui/metrics-scraper:v1.0.8\n",
      "🌟  Enabled addons: default-storageclass, storage-provisioner, dashboard\n",
      "🏄  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n"
     ]
    }
   ],
   "source": [
    "!minikube start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75dd7c",
   "metadata": {},
   "source": [
    "Let's view all of the pods running in the minikube cluster to make sure we can connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3262e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE\r\n",
      "kube-system            coredns-6d4b75cb6d-wrrwr                     1/1     Running   7 (9h ago)    2d10h\r\n",
      "kube-system            etcd-minikube                                1/1     Running   7 (9h ago)    2d10h\r\n",
      "kube-system            kube-apiserver-minikube                      0/1     Running   7 (9h ago)    2d10h\r\n",
      "kube-system            kube-controller-manager-minikube             0/1     Running   6 (9h ago)    2d10h\r\n",
      "kube-system            kube-proxy-5n4t9                             1/1     Running   7 (9h ago)    2d10h\r\n",
      "kube-system            kube-scheduler-minikube                      1/1     Running   6 (9h ago)    2d10h\r\n",
      "kube-system            storage-provisioner                          1/1     Running   12 (9h ago)   2d10h\r\n",
      "kubernetes-dashboard   dashboard-metrics-scraper-78dbd9dbf5-d4zv8   1/1     Running   4 (9h ago)    2d10h\r\n",
      "kubernetes-dashboard   kubernetes-dashboard-5fd5574d9f-7mjlt        1/1     Running   5 (9h ago)    2d10h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b757764",
   "metadata": {},
   "source": [
    "### Creating a Kubernetes Namespace\n",
    "\n",
    "Now that we have a cluster and are connected to it, we'll create a namespace to hold the resources for our model deployment. The resource definition is in the kubernetes/namespace.yaml file. To apply the manifest to the cluster, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48a8b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/model-services created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f ../kubernetes/namespace.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d804568",
   "metadata": {},
   "source": [
    "To take a look at the namespaces, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "471afbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   STATUS   AGE\r\n",
      "default                Active   2d10h\r\n",
      "kube-node-lease        Active   2d10h\r\n",
      "kube-public            Active   2d10h\r\n",
      "kube-system            Active   2d10h\r\n",
      "kubernetes-dashboard   Active   2d10h\r\n",
      "model-services         Active   2s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b135b8",
   "metadata": {},
   "source": [
    "The new namespace should appear in the listing along with other namespaces created by default by the system. To use the new namespace for the rest of the operations, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4528226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context \"minikube\" modified.\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl config set-context --current --namespace=model-services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68edb5a",
   "metadata": {},
   "source": [
    "### Creating the Redis Service\n",
    "\n",
    "Before we can deploy the model service we need to create the Redis service that will hold the cached predictions. For this service we will create a StatefulSet that manages two instances of the Redis service. We will use both instances from the decorator running in the model service.\n",
    "\n",
    "A StatefulSet is similar to a Deployment because it deploys Pods that are based on an identical specification. However, a StatefulSet will maintain an identity for each Pod and each one will be able to keep internal state. This is important because the Redis service is saving the cache for us, which is stateful. \n",
    "\n",
    "Using Redis in this manner is an example of sharding. Sharding is the process of splitting up data that is too big to fit into a single computer into multiple computers. By using sharding we can make our data layer distributed, which can make it more easily to scale in the future. \n",
    "\n",
    "A more detailed diagram of our software architecture looks like this:\n",
    "\n",
    "![Better Software Architecture](better_software_architecture_cfmlm.png)\n",
    "\n",
    "The Redis service is defined in the kubernetes/redis_service.yaml file. We can create it with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b853e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/redis-service created\r\n",
      "statefulset.apps/redis-st created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f ../kubernetes/redis_service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811059e",
   "metadata": {},
   "source": [
    "We can view the pods associated with this service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5b346505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redis-st-0   1/1     Running             0          4s\r\n",
      "redis-st-1   0/1     ContainerCreating   0          1s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods | grep redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd069dac",
   "metadata": {},
   "source": [
    "We wanted to create two instances of Redis in the StatefulSet, because the pods are part of a Stateful set their names end with a number and we will be able to reach individual pod from the model service.\n",
    "\n",
    "The .yaml file also created a Service for the StatefulSet pods which makes them accesible through DNS. We can view the service with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "81bd1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\r\n",
      "redis-service   ClusterIP   None         <none>        6379/TCP   7s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get services "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df810626",
   "metadata": {},
   "source": [
    "### Creating a Model Deployment and Service\n",
    "\n",
    "The model service now has a Redis instance to access, so we'll be creating the model service resources. These are:\n",
    "\n",
    "- Deployment: a declarative way to manage a set of pods, the model service pods are managed through the Deployment.\n",
    "- Service: a way to expose a set of pods in a Deployment, the model services is made available to the outside world through the Service, the service type is LoadBalancer which means that a load balancer will be created for the service.\n",
    "\n",
    "The model service pod requires an extra container running inside of it to enable easy access to the Redis service. Because we sharded the Redis service into two instances, the caching decorator would need to be aware of both instances of Redis in order to access the right one for each cache entry. We can avoid this by adding an ambassador service to the model service pod. An ambassador takes care of interactions between the application\n",
    "and any outside services. In this case, the ambassador container will take care of routing the cache request to the right Redis instance. We'll use [Twemproxy](https://github.com/twitter/twemproxy) to act as the ambassador between the model service and the Redis instances.\n",
    "\n",
    "The YAML for the ambassador container is defined in the Deployment resource of the model service and it looks like this:\n",
    "\n",
    "```yaml\n",
    "...\n",
    "- name: ambassador\n",
    "    image: malexer/twemproxy\n",
    "    env:\n",
    "      - name: REDIS_SERVERS\n",
    "        value: redis-st-0.redis-service.model-services.svc.cluster.local:6379:1,redis-st-1.redis-service.model-services.svc.cluster.local:6379:1\n",
    "    ports:\n",
    "      - containerPort: 6380\n",
    "...\n",
    "```\n",
    "\n",
    "Notice that the ambassador is listening on localhost port 6380. We'll need to set this correctly in the caching decorator's configuration.\n",
    "\n",
    "To start the model service, first we'll need to send the docker image from the local docker daemon to the minikube image cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "998382cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!minikube image load insurance_charges_model_service:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1bc4c",
   "metadata": {},
   "source": [
    "We can view the images in the minikube cache like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "46e92bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance_charges_model_service:latest\r\n"
     ]
    }
   ],
   "source": [
    "!minikube cache list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d68f1",
   "metadata": {},
   "source": [
    "The model service with the ambassador are created within the Kubernetes cluster with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0550078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/insurance-charges-model-deployment created\n",
      "service/insurance-charges-model-service created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../kubernetes/model_service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2019f58",
   "metadata": {},
   "source": [
    "The deployment and service for the model service were created together. You can see the new service with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd3e9032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance-charges-model-service   NodePort    10.107.94.124   <none>        80:32440/TCP   3s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get services | grep insurance-charges-model-service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b2763",
   "metadata": {},
   "source": [
    "Minikube exposes the service on a local port, we can get a link to the endpoint with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f1c504b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.59.100:32440\r\n"
     ]
    }
   ],
   "source": [
    "!minikube service insurance-charges-model-service --url -n model-services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cc2fd",
   "metadata": {},
   "source": [
    "To make a prediction, we'll hit the service with a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e072c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"charges\":25390.95}curl -X 'POST'  -H 'accept: application/json' -H  -d   0.01s user 0.01s system 8% cpu 0.158 total\r\n"
     ]
    }
   ],
   "source": [
    "!time curl -X 'POST' \\\n",
    "  'http://192.168.59.100:32440/api/models/insurance_charges_model/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{ \\\n",
    "        \\\"age\\\": 65, \\\n",
    "        \\\"sex\\\": \\\"male\\\", \\\n",
    "        \\\"bmi\\\": 22, \\\n",
    "        \\\"children\\\": 5, \\\n",
    "        \\\"smoker\\\": true, \\\n",
    "        \\\"region\\\": \\\"southwest\\\" \\\n",
    "    }\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a27d8",
   "metadata": {},
   "source": [
    "The service and decorator are working! The prediction request took 0.158 seconds. We'll try the same prediction one more time to see if it takes less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d7ae3cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"charges\":25390.95}curl -X 'POST'  -H 'accept: application/json' -H  -d   0.01s user 0.01s system 55% cpu 0.022 total\r\n"
     ]
    }
   ],
   "source": [
    "!time curl -X 'POST' \\\n",
    "  'http://192.168.59.100:32440/api/models/insurance_charges_model/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{ \\\n",
    "        \\\"age\\\": 65, \\\n",
    "        \\\"sex\\\": \\\"male\\\", \\\n",
    "        \\\"bmi\\\": 22, \\\n",
    "        \\\"children\\\": 5, \\\n",
    "        \\\"smoker\\\": true, \\\n",
    "        \\\"region\\\": \\\"southwest\\\" \\\n",
    "    }\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749aacc6",
   "metadata": {},
   "source": [
    "The second time we made the prediction it took 0.022 seconds, which is faster than the first time we made the prediction. This tells us that the caching is working as expected.\n",
    "\n",
    "We can review the contents of the Redis caches by executing the Redis CLI in the pods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d7ebb8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) \"0\"\r\n",
      "2) 1) \"insurance_charges_model/0.1.0/-4784352684431719157\"\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec --stdin --tty redis-st-1 -- redis-cli SCAN 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9583418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{\\\"charges\\\": 25390.95}\"\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec --stdin --tty redis-st-1 -- redis-cli GET insurance_charges_model/0.1.0/-4784352684431719157"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc1c45",
   "metadata": {},
   "source": [
    "Notice that the cache entry was found in the second instance of Redis in the StatefulSet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecebd080",
   "metadata": {},
   "source": [
    "### Adding a Prediction ID\n",
    "\n",
    "The model has a single decorator working on it within the model service but we can add any number of decorators to add functionality. In a [previous blog post](https://www.tekhnoal.com/ml-model-decorators.html) we created a decorator that added a unique prediction id to every prediction returned by the model. We can add this decorator to the service by simply changing the configuration:\n",
    "\n",
    "```yaml\n",
    "...\n",
    "decorators:\n",
    "  - class_path: data_enrichment.prediction_id.PredictionIDDecorator\n",
    "  - class_path: ml_model_caching.redis.RedisCachingDecorator\n",
    "    configuration:\n",
    "      host: \"localhost\"\n",
    "      port: 6380\n",
    "      database: 0\n",
    "      hashing_fields: \n",
    "        - age\n",
    "        - sex\n",
    "        - bmi\n",
    "        - children\n",
    "        - smoker\n",
    "        - region\n",
    "...\n",
    "```\n",
    "\n",
    "The PredictionIDDecorator decorator adds a unique identifier field to the prediction input data structure before the prediction request is passed to the caching decorator. We'll need to remove this field from the list of hashing fields because it should not be used to create the cached prediction, if we left the prediction_id field in the hashing fields then every single prediction request would be unique and we would not benefit from the cache.\n",
    "\n",
    "This configuration is in the ./configuration/kubernetes_rest_config2.yaml file. We'll change the configuration file being used and recreate the Deployment again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "108b8fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/insurance-charges-model-deployment configured\r\n",
      "service/insurance-charges-model-service unchanged\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../kubernetes/model_service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be43dbd",
   "metadata": {},
   "source": [
    "We'll try the service one more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "adc91e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"charges\":25390.95,\"prediction_id\":\"1aed2c71-9451-4cba-8d42-640d4b9695d8\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'POST' \\\n",
    "  'http://192.168.59.100:32440/api/models/insurance_charges_model/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{ \\\n",
    "        \\\"age\\\": 65, \\\n",
    "        \\\"sex\\\": \\\"male\\\", \\\n",
    "        \\\"bmi\\\": 22, \\\n",
    "        \\\"children\\\": 5, \\\n",
    "        \\\"smoker\\\": true, \\\n",
    "        \\\"region\\\": \\\"southwest\\\" \\\n",
    "    }\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c40da6",
   "metadata": {},
   "source": [
    "The service returned a unique identifier field called \"prediction_id\" along with the prediction. This field was generated by the decorator we added through configuration. A full explanation of how the prediction ID decorator works can be found in the previous blog post.\n",
    "\n",
    "This shows how easy and powerful it is to combine decorator with models in order to do more complex operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc13521d",
   "metadata": {},
   "source": [
    "### Deleting the Resources\n",
    "\n",
    "Now that we're done with the service we need to destroy the resources. To delete the Redis deploymet, we'll delete the kubernetes resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "395bb7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service \"redis-service\" deleted\r\n",
      "statefulset.apps \"redis-st\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f ../kubernetes/redis_service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07847644",
   "metadata": {},
   "source": [
    "To delete the model service, we'll execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "87d50afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"insurance-charges-model-deployment\" deleted\r\n",
      "service \"insurance-charges-model-service\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f ../kubernetes/model_service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca534d",
   "metadata": {},
   "source": [
    "To delete the namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "31de660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace \"model-services\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f ../kubernetes/namespace.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94fa6a",
   "metadata": {},
   "source": [
    "Lastly, to stop the kubernetes cluster, execute these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "15adf1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✋  Stopping node \"minikube\"  ...\n",
      "🛑  1 node stopped.\n"
     ]
    }
   ],
   "source": [
    "!minikube stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b76047",
   "metadata": {},
   "source": [
    "## Closing\n",
    "\n",
    "In this blog post, we showed how to build a decorator class that is able to cache predictions made by a machine learning model. Caching is a simple way to speed up predictions that we know can be reused and are requested often from a model. \n",
    "\n",
    "The cache decorator classes can be applied to any model that uses the MLModel base class without having to modify the model class at all. The caching functionality is contained completely in the RedisCacheDecorator class. The same thing is true for the RESTful model service, the cache functionality did not need to be added to the service because we separated the concerns of the service and the cache decorator. We were able to add caching to the deployed model by modifying the configuration. By using decorators we’re able to create software components that can be reused in many different contexts. For example, if we chose to deploy the cache decorator in a gRPC service we should be able to do so as long as we instantiate and manage the decorator instance correctly.\n",
    "\n",
    "Combining the caching decorator with other decorators that require I/O like data enrichment is very easy because of the way that decorators can be \"stacked\" together. We showed how to do this in this blog post by adding a decorator that adds unique identifier to each prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
