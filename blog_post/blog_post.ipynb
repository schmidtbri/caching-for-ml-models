{
 "cells": [
  {
   "cell_type": "raw",
   "id": "97711a24",
   "metadata": {},
   "source": [
    "Title: Caching for ML Model Deployments\n",
    "Date: 2022-07-01 07:00\n",
    "Category: Blog\n",
    "Slug: caching-for-ml-models\n",
    "Authors: Brian Schmidt\n",
    "Summary: In a software system, a [cache](https://en.wikipedia.org/wiki/Cache_(computing)) is a data store that is used to temporarily store computation results or frequently-accessed data. When accessing the results of a computation from a cache, we are able to avoid paying the cost of recomputing the result. When accessing a frequently accessed piece of data we are able to avoid paying the cost of accessing the data from a slower data store, this type of caching is used when accessing data from a slower data store than the cache. When a cache hit occurs, the data being sought is found and returned to the caller. When a “miss” occurs, the data is not found and must be recomputed or accessed from the slower data store by the caller. A data cache is generally built using storage that has low latency, which means that it is more expensive to run. Machine learning model deployments can benefit from caching because making predictions with a model is usually a CPU-bound process, especially for large and complex models. Predictions that take a long time to make can be cached and returned later when the same prediction is requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca330c8",
   "metadata": {},
   "source": [
    "# Caching for ML Model Deployments\n",
    "\n",
    "In a [previous blog post]() we introduced the decorator pattern for ML model deployments and then showed how to use the pattern to build extensions to a normal deployment. For example, in [this blog post]() we added data enrichment, in [this blog post]() we added logging, in [this blog post]() we added metrics, and in [this blog post]() we added distributed tracing. All of these extensions were added without having to modify the machine learning model code at all, we were able to do it by using the decorator pattern. In this blog post we’ll add caching functionality to a model in the same way.\n",
    "\n",
    "This blog post is written in a Jupyter notebook, some of the code and commands found in it reflects this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44522dc6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In a software system, a [cache](https://en.wikipedia.org/wiki/Cache_(computing)) is a data store that is used to temporarily store computation results or frequently-accessed data. When accessing the results of a computation from a cache, we are able to avoid paying the cost of recomputing the result. When accessing a frequently accessed piece of data we are able to avoid paying the cost of accessing the data from a slower data store, this type of caching is used when accessing data from a slower data store than the cache. When a cache hit occurs, the data being sought is found and returned to the caller. When a “miss” occurs, the data is not found and must be recomputed or accessed from the slower data store by the caller. A data cache is generally built using storage that has low latency, which means that it is more expensive to run. \n",
    "\n",
    "Machine learning model deployments can benefit from caching because making predictions with a model is usually a CPU-bound process, especially for large and complex models. Predictions that take a long time to make can be cached and returned later when the same prediction is requested. This type of caching is also known as [memoization](https://en.wikipedia.org/wiki/Memoization).\n",
    "\n",
    "In order to enable prediction caching possible from ML models, we need to make sure that the model can produce deterministic predictions. Determinism is a property of algorithms that says that the algorithm will always return the same output for the same input. If the model for which we want to cache predictions returns a different prediction for the same inputs, then we wouldn’t be able to cache the predictions at all since we wouldn’t be able to guarantee that the model would return the same prediction that we had cached.\n",
    "\n",
    "The effectiveness of a cache is measured by the cache hit ratio, which is defined as the number of times a piece of needed data is found in the cache, divided by the total number of times a piece of data is requested from the cache. A bigger cache can hold more data which means it can have a higher hit ratio, but it will cost more money to run. The size of a cache must be balanced with the performance benefits that it provides to make the economics make sense.\n",
    "\n",
    "Another aspect of cache performance is the amount of time that is saved when using the cache to access data. A cache is only a net benefit if the time saved during cache hits exceeds the time lost from the additional overhead of recalculating the needed results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d69be",
   "metadata": {},
   "source": [
    "## Software Architecture\n",
    "\n",
    "![Software Architecture](software_architecture_cfmlm.png)\n",
    "![Software Architecture]({attach}software_architecture_cfmlm.png){ width=100% }\n",
    "\n",
    "For caching predictions, we’ll be using [Redis](https://en.wikipedia.org/wiki/Redis). Redis is a data structure store that allows users to save and modify data structures in a remote service. This allows many clients to safely access the same data from a centralized service. Redis supports many different data structures, but we’ll be using the key-value store functionality to save our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdcd200",
   "metadata": {},
   "source": [
    "## Installing the Model\n",
    "\n",
    "To make this blog post a little shorter we won't train a completely new model. Instead we'll install a model that we've [built in a previous blog post](https://www.tekhnoal.com/regression-model.html). The code for the model is in [this github repository](https://github.com/schmidtbri/regression-model).\n",
    "\n",
    "To install the model, we can use the pip command and point it at the github repo of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f1b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "!pip install -e git+https://github.com/schmidtbri/regression-model#egg=insurance_charges_model\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f42fb",
   "metadata": {},
   "source": [
    "To make a prediction with the model, we'll import the model's class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58eb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurance_charges_model.prediction.model import InsuranceChargesModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516c054",
   "metadata": {},
   "source": [
    "Now we can instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84644224",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceChargesModel()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f4b41",
   "metadata": {},
   "source": [
    "To make a prediction, we'll need to use the model's input schema class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurance_charges_model.prediction.schemas import InsuranceChargesModelInput, \\\n",
    "    SexEnum, RegionEnum\n",
    "\n",
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5332f84",
   "metadata": {},
   "source": [
    "The model's input schema is called InsuranceChargesModelInput and it encompasses all of the features required by the model to make a prediction.\n",
    "\n",
    "Now we can make a prediction with the model by calling the predict() method with an instance of the InsuranceChargesModelInput class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b66f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30620b8",
   "metadata": {},
   "source": [
    "The model predicts that the charges will be $8640.78.\n",
    "\n",
    "When deploying the model we’ll pretend that the age, sex, bmi, children, smoker, and region fields are not available from the client system that is calling the model. Because of this, we’ll need to add it to the model input by loading the data from the database.\n",
    "\n",
    "We can view input schema of the model as a JSON schema document by calling the .schema() method on the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc015ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'InsuranceChargesModelInput',\n",
       " 'description': \"Schema for input of the model's predict method.\",\n",
       " 'type': 'object',\n",
       " 'properties': {'age': {'title': 'Age',\n",
       "   'description': 'Age of primary beneficiary in years.',\n",
       "   'minimum': 18,\n",
       "   'maximum': 65,\n",
       "   'type': 'integer'},\n",
       "  'sex': {'title': 'Sex',\n",
       "   'description': 'Gender of beneficiary.',\n",
       "   'allOf': [{'$ref': '#/definitions/SexEnum'}]},\n",
       "  'bmi': {'title': 'Body Mass Index',\n",
       "   'description': 'Body mass index of beneficiary.',\n",
       "   'minimum': 15.0,\n",
       "   'maximum': 50.0,\n",
       "   'type': 'number'},\n",
       "  'children': {'title': 'Children',\n",
       "   'description': 'Number of children covered by health insurance.',\n",
       "   'minimum': 0,\n",
       "   'maximum': 5,\n",
       "   'type': 'integer'},\n",
       "  'smoker': {'title': 'Smoker',\n",
       "   'description': 'Whether beneficiary is a smoker.',\n",
       "   'type': 'boolean'},\n",
       "  'region': {'title': 'Region',\n",
       "   'description': 'Region where beneficiary lives.',\n",
       "   'allOf': [{'$ref': '#/definitions/RegionEnum'}]}},\n",
       " 'definitions': {'SexEnum': {'title': 'SexEnum',\n",
       "   'description': \"Enumeration for the value of the 'sex' input of the model.\",\n",
       "   'enum': ['male', 'female'],\n",
       "   'type': 'string'},\n",
       "  'RegionEnum': {'title': 'RegionEnum',\n",
       "   'description': \"Enumeration for the value of the 'region' input of the model.\",\n",
       "   'enum': ['southwest', 'southeast', 'northwest', 'northeast'],\n",
       "   'type': 'string'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6e934",
   "metadata": {},
   "source": [
    "## Profiling the Model\n",
    "\n",
    "In order to get an idea of how much time it takes for our model to make a prediction, we'll profile it by making predictions with ranom data. To do this, we'll use the [Faker package](https://faker.readthedocs.io/en/master/). We can install it with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f9e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Faker\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be4bf7",
   "metadata": {},
   "source": [
    "We'll create a function that can generate a random sample that meets the model's input schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1fd8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "def generate_record() -> InsuranceChargesModelInput:\n",
    "    record = {\n",
    "        \"age\": faker.random_int(min=18, max=65),\n",
    "        \"sex\": faker.random_choices(elements=(\"male\", \"female\"), length=1)[0],\n",
    "        \"bmi\": faker.random_int(min=15000, max=50000)/1000.0,\n",
    "        \"children\": faker.random_int(min=0, max=5),\n",
    "        \"smoker\": faker.boolean(),\n",
    "        \"region\": faker.random_choices(elements=(\"southwest\", \"southeast\", \"northwest\", \"northeast\"), length=1)[0]\n",
    "    }\n",
    "    return InsuranceChargesModelInput(**record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774def2",
   "metadata": {},
   "source": [
    "The function returns an instance of the InsuranceChargesModelInput class, which is the type required by the model's predict() method. We'll use this function to profile the predict() method of the model.\n",
    "\n",
    "It's really hard to see a performance difference with one sample, so we'll perform a test with many random samples to see the difference. To start, we'll generate 1000 samples and save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c9f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    samples.append(generate_record())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92078c8f",
   "metadata": {},
   "source": [
    "By using the timeit module from the standard library, we can measure how much time it takes to call the model's predict method with a random sample generated by the generate_record() function. We'll call the method 1000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0319ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "total_seconds = timeit.timeit(\"[model.predict(sample) for sample in samples]\", number=1, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323e67a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The model took 33.307 seconds to perform 1000 predictions, therefore it took 0.0333 seconds to make a single prediction. \n",
       "\n",
       "The model takes about 33.307 milliseconds to make a prediction"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seconds_per_sample = total_seconds / 1000.0\n",
    "milliseconds_per_sample = seconds_per_sample * 1000.0\n",
    "\n",
    "md(\"The model took {} seconds to perform 1000 predictions, therefore it took \"\n",
    "   \"{} seconds to make a single prediction. \\n\\nThe model takes about {} milliseconds to \"\n",
    "   \"make a prediction\".format(round(total_seconds, 3), \n",
    "                              round(seconds_per_sample, 4), \n",
    "                              round(milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326d315",
   "metadata": {},
   "source": [
    "## Hashing Model Inputs\n",
    "\n",
    "Before we can build a caching decorator, we'll need to understand a little bit about hashing and how to use it for caching. A hashing operation is an operation takes in data of arbritrary size as input and returns data of a fixed size. A \"hash\" value refers to the fixed-size data that is returned from a hashing operation. Hashing has many uses in computer science, in this application we'll us hashing to uniquely identify some inputs that are provided to the ML model that we are decorating.\n",
    "\n",
    "Hashing is already built into the Python standard library through the hash() function, but it is only supported on certain types of objects. We can try it out using an instance of the model's input schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b709239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1297609423663202376"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92d0eb",
   "metadata": {},
   "source": [
    "To try out hashing, we converted an instance of the model's input schema into a dictionary, and then converted the keys and values of the dictionary into [frozensets](https://docs.python.org/3/library/stdtypes.html#frozenset). We then used the frozensets with the hash() function to create an integer value. The integer is the hashed value that we need to uniquely identify the inputs to the model.\n",
    "\n",
    "To see how hashing works, we'll create a separate input instance for the model that has the exact same values and hash it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5b66b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1297609423663202376"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62742c9",
   "metadata": {},
   "source": [
    "The hashed values are exactly the same, as we expected. The hashes value should be different if any of the values in the model input change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf041687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7143663760078629168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.2,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095d0c7",
   "metadata": {},
   "source": [
    "The \"bmi\" field changed from 24.0 to 24.2, so we got a completely different hashed value.\n",
    "\n",
    "Hashing is a quick and easy way to identify inputs which will allow us to store the predictions of the model in the cache and retrieve them later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f958fb7",
   "metadata": {},
   "source": [
    "## Creating the Redis Cache Decorator\n",
    "\n",
    "We'll be using Redis to hold the cached predictions of the model. To access the Redis instance, we'll use the redis python package, which we'll install with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c01f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install redis\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b728e",
   "metadata": {},
   "source": [
    "Now we can implement the decorator class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "574152ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from ml_base.decorator import MLModelDecorator\n",
    "import redis\n",
    "import json\n",
    "\n",
    "\n",
    "class RedisCachingDecorator(MLModelDecorator):\n",
    "    \"\"\"Decorator for caching around an MLModel instance.\"\"\"\n",
    "\n",
    "    def __init__(self, host: str, port: str, database: str, prefix: Optional[str] = None, \n",
    "                 hashing_fields: Optional[List[str]] = None) -> None:\n",
    "        \n",
    "        super().__init__(host=host, port=port, database=database, prefix=prefix, \n",
    "                         hashing_fields=hashing_fields)\n",
    "        \n",
    "        self.__dict__[\"_redis_client\"] = redis.Redis(host=host, port=port, db=database)\n",
    "\n",
    "    def predict(self, data):\n",
    "        if self._configuration[\"prefix\"] is not None:\n",
    "            prefix = \"{}/{}/{}/\".format(self._configuration[\"prefix\"], \n",
    "                                        self._model.qualified_name, \n",
    "                                        self._model.version)\n",
    "        else:\n",
    "            prefix = \"{}/{}/\".format(self._model.qualified_name, \n",
    "                                     self._model.version)\n",
    "\n",
    "        # select hashing fields from input\n",
    "        if self._configuration[\"hashing_fields\"] is not None:\n",
    "            data_dict = {key: data.dict()[key] for key in self._configuration[\"hashing_fields\"]}\n",
    "        else:\n",
    "            data_dict = data.dict()\n",
    "        \n",
    "        # creating a key for the prediction inputs provided\n",
    "        frozen_data = frozenset(data_dict.keys()), frozenset(data_dict.values())\n",
    "        key = prefix + str(hash(frozen_data))\n",
    "       \n",
    "        # check if the prediction is in the cache\n",
    "        prediction = self.__dict__[\"_redis_client\"].get(key)\n",
    "        \n",
    "        # if the prediction is present in the cache, then deserialize it and return the prediction\n",
    "        if prediction is not None:\n",
    "            prediction = json.loads(prediction)\n",
    "            prediction = self._model.output_schema(**prediction)\n",
    "            return prediction\n",
    "        # if the prediction is not present in the cache, then make a prediction, save it to the cache, and return the prediction\n",
    "        else:\n",
    "            prediction = self._model.predict(data)\n",
    "            serialized_prediction = json.dumps(prediction.dict())\n",
    "            self.__dict__[\"_redis_client\"].set(key, serialized_prediction)\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffdd07",
   "metadata": {},
   "source": [
    "The caching decorator works very simply, when it receives inputs for the model it:\n",
    "\n",
    "- creates a key for the data structures\n",
    "- checks if the key is present in the cache\n",
    "- if the key is present:\n",
    "    - retrieves the prediction for that key \n",
    "    - deserializes the contents of the cache into the output type of the model\n",
    "    - returns the prediction to the caller\n",
    "- if the key is not present:\n",
    "    - makes a prediction with the model it is decorating\n",
    "    - serializes the prediction to a JSON string\n",
    "    - saves the prediction to the cache with the key generated\n",
    "    - returns the prediction to the caller\n",
    "\n",
    "The key created for each cache entry is made up of the model's qualified name, the model version and an optional prefix. The prefix is optional and is used to differentiate the predictions that are cached in a more flexible way. The caching decorator uses JSON as a serialization format to store information in the cache. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc173903",
   "metadata": {},
   "source": [
    "## Using the Redis Cache Decorator\n",
    "\n",
    "In order to try out the decorator, we'll need to run a local Redis instance. We can start one using Docker with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3403e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa2cfbefa2290fcc314aeaeab9935ce92dabcc650becb9807319ef90473a292e\r\n"
     ]
    }
   ],
   "source": [
    "!docker run -d -p 6379:6379 --name local-redis redis/redis-stack-server:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b645fb8",
   "metadata": {},
   "source": [
    "To test out the decorator we first need to instantiate the model object that we want to use with the decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67724bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceChargesModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846001e",
   "metadata": {},
   "source": [
    "Next, we’ll instantiate the decorator with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8a71a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                          port=6379,\n",
    "                                          database=0,\n",
    "                                          prefix=\"prefix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f722a9",
   "metadata": {},
   "source": [
    "We can add the model instance to the decorator after it’s been instantiated like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec74d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_model = caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58efd0",
   "metadata": {},
   "source": [
    "We can see the decorator and the model objects by printing the reference to the decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d36281f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RedisCachingDecorator(InsuranceChargesModel)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decorated_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305be27c",
   "metadata": {},
   "source": [
    "The decorator object is printing out it's own type along with the type of the model that it is decorating.\n",
    "\n",
    "Now we’ll try to use the decorator and the model together by making a few predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1455952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=9612.64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=46,\n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03220352",
   "metadata": {},
   "source": [
    "The first time we make a prediction with a given input, we'll get the prediction made by the model and the decorator will store the prediction in the cache. \n",
    "\n",
    "We can view the key in the redis database to see how it is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "529cd329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "prefix/insurance_charges_model/0.1.0/6204525449924069161\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli SCAN 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bee5f1",
   "metadata": {},
   "source": [
    "There is a single key in the redis database. We'll access they key like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7674a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli GET prefix/insurance_charges_model/0.1.0/198181040256854193"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4834f3",
   "metadata": {},
   "source": [
    "The prediction is stored in the key as a JSON string.\n",
    "\n",
    "We'll try the same prediction again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea103734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=9612.64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=46, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcdb8c",
   "metadata": {},
   "source": [
    "This time the prediction was not made by the model, it was found in the Redis cache and returned by the decorator instead of being made again.\n",
    "\n",
    "Next, we'll use the samples to make predictions with the decorated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6224b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_total_seconds = timeit.timeit(\"[decorated_model.predict(sample) for sample in samples]\", number=1, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9695ff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorated model took 35.53 seconds to perform 1000 predictions the first time that it saw the prediction inputs, therefore it took 0.0355 seconds to make a single prediction. \n",
       "\n",
       "The decorated model takes about 35.53 milliseconds to make a prediction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decorated_seconds_per_sample = decorated_total_seconds / 1000.0\n",
    "decorated_milliseconds_per_sample = decorated_seconds_per_sample * 1000.0\n",
    "\n",
    "md(\"The decorated model took {} seconds to perform 1000 predictions the first time that it saw \"\n",
    "   \"the prediction inputs, therefore it took {} seconds to make a single prediction. \"\n",
    "   \"\\n\\nThe decorated model takes about {} milliseconds to make a prediction.\".\n",
    "   format(round(decorated_total_seconds, 3), \n",
    "          round(decorated_seconds_per_sample, 4), \n",
    "          round(decorated_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf40f52",
   "metadata": {},
   "source": [
    "We'll run the same samples through again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d48f335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorated_total_seconds = timeit.timeit(\"[decorated_model.predict(sample) for sample in samples]\", number=1, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c748998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorated model took 0.894 seconds to perform 1000 predictions the second time that it saw the prediction inputs, therefore it took 0.0009 seconds to make a single prediction. \n",
       "\n",
       "The model takes about 0.894 milliseconds to make a prediction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decorated_seconds_per_sample = decorated_total_seconds / 1000.0\n",
    "decorated_milliseconds_per_sample = decorated_seconds_per_sample * 1000.0\n",
    "\n",
    "md(\"The decorated model took {} seconds to perform 1000 predictions the second time that it saw \"\n",
    "   \"the prediction inputs, therefore it took {} seconds to make a single prediction. \"\n",
    "   \"\\n\\nThe model takes about {} milliseconds to make a prediction.\".\n",
    "   format(round(decorated_total_seconds, 3), \n",
    "          round(decorated_seconds_per_sample, 4), \n",
    "          round(decorated_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287da0d",
   "metadata": {},
   "source": [
    "It took less time because the cached predictions were returned more quickly because we requested the same predictions from the model.\n",
    "\n",
    "We can get the amount of memory used by the cache by accessing the keys and summing up the length of the byte array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bf0adca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The keys in the cache take up a total of 20630 bytes. The average number of bytes per cache entry is 20.61."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "\n",
    "decorated_number_of_bytes = 0\n",
    "decorated_total_entries = 0\n",
    "for key in r.scan_iter(\"prefix*\"):\n",
    "    decorated_number_of_bytes += len(r.get(key))\n",
    "    decorated_total_entries = decorated_total_entries + 1\n",
    "    \n",
    "decorated_average_number_of_bytes = decorated_number_of_bytes / decorated_total_entries\n",
    "    \n",
    "md(\"The keys in the cache take up a total of {} bytes. \"\n",
    "   \"The average number of bytes per cache entry is {}.\"\n",
    "   .format(decorated_number_of_bytes, \n",
    "           round(decorated_average_number_of_bytes, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa033e5",
   "metadata": {},
   "source": [
    "We'll clear the redis database to make sure the contents don't intefere with the next things we want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e47aaf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b61cb7",
   "metadata": {},
   "source": [
    "## Selecting Fields For Hashing\n",
    "\n",
    "In certain situations, not all of the fields in the model's input should be used to create a hash. This may be because not all of the model's input fields are actually used for making a prediction. Some fields may be used for logging or debugging and do not actually affect the prediction created by the model. If changing the value of a field does not affect the value of the prediction created by the model, it should not be used to create the hashed key for the cache.\n",
    "\n",
    "The caching decorator supports selecting specific fields from the input to create the cache key. The option is called \"hashing_fields\" and is provided to the decorator instance like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7466fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                          port=6379,\n",
    "                                          database=0,\n",
    "                                          prefix=\"prefix\",\n",
    "                                          hashing_fields=[\"age\", \"sex\", \"bmi\", \"children\", \"smoker\"])\n",
    "\n",
    "decorated_model = caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681df7bf",
   "metadata": {},
   "source": [
    "The decorator now uses all of the input fields except for the \"region\" field to create the key.\n",
    "\n",
    "To try out the functionality, we'll create a prediction with the decorated model. The prediction will get saved in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16f00b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15219.19)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=52, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=3,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630c48d",
   "metadata": {},
   "source": [
    "We'll now make the same prediction, but this time the prediction will come from the cache because it was saved there previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f891b11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15219.19)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=52, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=3,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516dcf9",
   "metadata": {},
   "source": [
    "We'll make the prediction one more time, but this time we'll change the value of the \"region\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6892a4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15219.19)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=52, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.0,\n",
    "    children=3,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.southeast)\n",
    "\n",
    "prediction = decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ec9ca",
   "metadata": {},
   "source": [
    "The predicted value should have changed because the region changed. It didn't change because we accessed the prediction from the cache instead of creating a new one. This happened because we ignored the value of the \"region\" field when creating the hashed key in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9173c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9aacfe",
   "metadata": {},
   "source": [
    "## Improving the Performance of the Decorator\n",
    "\n",
    "When a prediction is stored in the cache, it is currently serialized using the JSON format. This format is simple and easy to understand, but it is not the most efficient format for serialization in terms of the size of the data and the time it takes to do the serialization.\n",
    "\n",
    "To try to improve the efficiency of the caching decorator we'll add options for other serialization formats and also try to use compression. Another way to reduce the memory usage of the cache is to reduce the precision of the numbers given to the model. These approaches will be fully explained below.\n",
    "\n",
    "We'll be using [MessagePack](https://msgpack.org/index.html) to to serialization, so we need to install the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09e1b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install msgpack\n",
    "!pip install python-snappy\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed523cce",
   "metadata": {},
   "source": [
    "We'll recreate the RedisCachingDecorator class with the code needed to support the new features we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78e1e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "import snappy\n",
    "\n",
    "\n",
    "class RedisCachingDecorator(MLModelDecorator):\n",
    "    \"\"\"Decorator for caching around an MLModel instance.\"\"\"\n",
    "\n",
    "    def __init__(self, host: str, port: str, database: str, prefix: Optional[str] = None, \n",
    "                 hashing_fields: Optional[List[str]] = None, serder: str = \"JSON\", \n",
    "                 use_compression: bool = False, \n",
    "                 reduced_precision_fields: Optional[List[str]] = None,\n",
    "                 number_of_places: Optional[int] = None\n",
    "                ) -> None:\n",
    "        \n",
    "        if serder not in [\"JSON\", \"MessagePack\"]:\n",
    "            raise ValueError(\"Serder option not supported.\")\n",
    "            \n",
    "        if reduced_precision_fields is None and number_of_places is not None:\n",
    "            raise ValueError(\"number_of_places must be provided when reduced_precision_fields is provided.\")\n",
    "            \n",
    "        if number_of_places is None and reduced_precision_fields is not None:\n",
    "            raise ValueError(\"reduced_precision_fields must be provided when number_of_places is provided.\")\n",
    "        \n",
    "        super().__init__(host=host, port=port, database=database, prefix=prefix, \n",
    "                         hashing_fields=hashing_fields, serder=serder, \n",
    "                         use_compression=use_compression, \n",
    "                         reduced_precision_fields=reduced_precision_fields,\n",
    "                         number_of_places=number_of_places)\n",
    "        \n",
    "        self.__dict__[\"_redis_client\"] = redis.Redis(host=host, port=port, db=database)\n",
    "\n",
    "    def predict(self, data):\n",
    "        if self._configuration[\"prefix\"] is not None:\n",
    "            prefix = \"{}/{}/{}/\".format(self._configuration[\"prefix\"], \n",
    "                                        self._model.qualified_name, \n",
    "                                        self._model.version)\n",
    "        else:\n",
    "            prefix = \"{}/{}/\".format(self._model.qualified_name,\n",
    "                                     self._model.version)\n",
    "        #print(data)\n",
    "        if self._configuration[\"reduced_precision_fields\"] is not None:\n",
    "                for field_name, field_value in data.dict().items():\n",
    "                    if field_name in self._configuration[\"reduced_precision_fields\"]:\n",
    "                        setattr(data, field_name, round(field_value, self._configuration[\"number_of_places\"]))\n",
    "        #print(data)\n",
    "\n",
    "        # select hashing fields from input\n",
    "        if self._configuration[\"hashing_fields\"] is not None:\n",
    "            data_dict = {key: data.dict()[key] for key in self._configuration[\"hashing_fields\"]}\n",
    "        else:\n",
    "            data_dict = data.dict()\n",
    "        \n",
    "        # creating a key for the prediction inputs provided\n",
    "        frozen_data = frozenset(data_dict.keys()), frozenset(data_dict.values())\n",
    "        key = prefix + str(hash(frozen_data))\n",
    "       \n",
    "        # check if the prediction is in the cache\n",
    "        prediction = self.__dict__[\"_redis_client\"].get(key)\n",
    "        \n",
    "        # if the prediction is present in the cache\n",
    "        if prediction is not None:\n",
    "\n",
    "            # optionally decompressing the bytes\n",
    "            if self._configuration[\"use_compression\"]:\n",
    "                decompressed_prediction = snappy.decompress(prediction)\n",
    "            else:\n",
    "                decompressed_prediction = prediction\n",
    "            \n",
    "            # deserializing to bytes\n",
    "            if self._configuration[\"serder\"] == \"JSON\":\n",
    "                deserialized_prediction = json.loads(decompressed_prediction.decode())\n",
    "            elif self._configuration[\"serder\"] == \"MessagePack\":\n",
    "                deserialized_prediction = msgpack.loads(decompressed_prediction)\n",
    "            else: \n",
    "                raise ValueError(\"Serder option not supported.\")\n",
    "                \n",
    "            # creating the output instance\n",
    "            prediction = self._model.output_schema(**deserialized_prediction)\n",
    "\n",
    "            return prediction\n",
    "\n",
    "        # if the prediction is not present in the cache\n",
    "        else:\n",
    "            # making a prediction with the model\n",
    "            prediction = self._model.predict(data)\n",
    "\n",
    "            # serializing to bytes\n",
    "            if self._configuration[\"serder\"] == \"JSON\":\n",
    "                serialized_prediction = str.encode(json.dumps(prediction.dict()))\n",
    "            elif self._configuration[\"serder\"] == \"MessagePack\":\n",
    "                serialized_prediction = msgpack.dumps(prediction.dict())\n",
    "            else: \n",
    "                raise ValueError(\"Serder option not supported.\")\n",
    "                \n",
    "            # optionally compressing the bytes\n",
    "            if self._configuration[\"use_compression\"]:\n",
    "                serialized_prediction = snappy.compress(serialized_prediction)\n",
    "                \n",
    "            # saving the prediction to the cache\n",
    "            self.__dict__[\"_redis_client\"].set(key, serialized_prediction)\n",
    "\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32f733",
   "metadata": {},
   "source": [
    "The new implementation above includes options to enable MessagePack for serialization/deserialization, snappy for compression, and the ability to reduce the precision of numerical fields in the model input. We'll try out each option individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f655bf3",
   "metadata": {},
   "source": [
    "### MessagePack Serialization\n",
    "\n",
    "An alternative serialization format is [MessagePack](https://msgpack.org/index.html). This format is a binary serialization format designed for small and efficient and flexible serialization. \n",
    "\n",
    "To enable MessagePack, we'll instantiate the decorator setting the \"serder\" option to \"MessagePack\". We'll use a prefix to separate the cache entries that use MessagePack from the other cache entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc3bee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpack_caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                                  port=6379,\n",
    "                                                  database=0,\n",
    "                                                  prefix=\"msgpack\",\n",
    "                                                  serder=\"MessagePack\")\n",
    "\n",
    "mspgpack_decorated_model = msgpack_caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712fe66",
   "metadata": {},
   "source": [
    "The first time we make a prediction, the model will be used and the prediction will get serialized to MessagePack and saved to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddd14e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15113.29)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=55, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=25.0,\n",
    "    children=4,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = mspgpack_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104bde4",
   "metadata": {},
   "source": [
    "The second time we make a prediction, the cache entry will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e91aed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15113.29)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=55, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=25.0,\n",
    "    children=4,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = mspgpack_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaeed6",
   "metadata": {},
   "source": [
    "The MessagePack format works, now we'll do some testing to see if it improves the serialization/deserialization performance.\n",
    "\n",
    "As before, we'll make the predictions on the samples to fill in the cache with predictions. We'll be using the 1000 samples generated above to keep the comparison fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "456cc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpack_total_seconds = timeit.timeit(\"[mspgpack_decorated_model.predict(sample) for sample in samples]\", number=1, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83f83e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The model that uses MessagePack took 35.817 seconds to perform 1000 predictions the first time that it saw the prediction inputs. The model takes about 0.0358 milliseconds to make a prediction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgpack_seconds_per_sample = msgpack_total_seconds / 1000.0\n",
    "msgpack_milliseconds_per_sample = msgpack_seconds_per_sample * 1000.0\n",
    "\n",
    "md(\"The model that uses MessagePack took {} seconds to perform 1000 predictions the first time that it saw \"\n",
    "   \"the prediction inputs. The model takes about {} milliseconds to make a prediction.\".\n",
    "   format(round(msgpack_total_seconds, 3), \n",
    "          round(msgpack_seconds_per_sample, 4), \n",
    "          round(msgpack_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2fb42",
   "metadata": {},
   "source": [
    "Most of the time for this step is taken up by the model's prediction algorithm, this is the reason why its a similar amount of time as the JSON serder we used before.\n",
    "\n",
    "Now we can try the same predictions again. This time, they'll be accessed from the cache and returned more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1839a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpack_total_seconds = timeit.timeit(\"[mspgpack_decorated_model.predict(sample) for sample in samples]\", number=1, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3de1f271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The model that uses MessagePack took 0.927 seconds to perform 1000 predictions the second time that it saw the prediction inputs. The model takes about 0.001 milliseconds to make a prediction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgpack_seconds_per_sample = msgpack_total_seconds / 1000.0\n",
    "msgpack_milliseconds_per_sample = msgpack_seconds_per_sample * 1000.0\n",
    "\n",
    "md(\"The model that uses MessagePack took {} seconds to perform 1000 predictions the second time that it saw \"\n",
    "   \"the prediction inputs. The model takes about {} milliseconds to make a prediction.\".\n",
    "   format(round(msgpack_total_seconds, 3), \n",
    "          round(msgpack_seconds_per_sample, 3), \n",
    "          round(msgpack_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20b5f5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The MessagePack serder performs slightly better than the JSON serder. The test we did with JSON above took about 0.8935623240000012 ms for each sample, the MessagePack serder took 0.9270776179999984 ms per sample."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The MessagePack serder performs slightly better than the JSON serder. \"\n",
    "   \"The test we did with JSON above took about {} ms for each sample, \"\n",
    "   \"the MessagePack serder took {} ms per sample.\"\n",
    "   .format(round(decorated_milliseconds_per_sample, 3), \n",
    "           round(msgpack_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd09f8",
   "metadata": {},
   "source": [
    "We can see how much space the cache entries is taking up by querying each key and summing up the number of bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfe522f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The keys in the original JSON cache took up a total of 20630 bytes. The keys in the MessagePack cache take up a total of 18018 bytes and the average number of bytes per MessagePack cache entry is 18.0."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgpack_number_of_bytes = 0\n",
    "msgpack_total_entries = 0\n",
    "for key in r.scan_iter(\"msgpack*\"):\n",
    "    msgpack_number_of_bytes += len(r.get(key))\n",
    "    msgpack_total_entries = msgpack_total_entries + 1\n",
    "    \n",
    "msgpack_average_number_of_bytes = msgpack_number_of_bytes / msgpack_total_entries\n",
    "\n",
    "md(\"The keys in the original JSON cache took up a total of {} bytes. \"\n",
    "   \"The keys in the MessagePack cache take up a total of {} bytes and the \"\n",
    "   \"average number of bytes per MessagePack cache entry is {}.\"\n",
    "   .format(decorated_number_of_bytes,\n",
    "           msgpack_number_of_bytes, \n",
    "           msgpack_average_number_of_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e44f5",
   "metadata": {},
   "source": [
    "By using MessagePack serialization we were able to save memory in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f274c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6c860",
   "metadata": {},
   "source": [
    "### Snappy Compression\n",
    "\n",
    "[Snappy](https://github.com/google/snappy) is a compression algorithm built by Google that targets high compression ratios and high compressions speed. We can try to reduce the memory used by the cache by compressing the cache entries with the Snappy algorithm. This approach was inspired by [another blog post](https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/).\n",
    "\n",
    "Enabling compression on the decorator is very simple, we'll just set the \"use_compression\" parameter to \"True\" when instantiating the caching decorator. In this example we'll use JSON serialization combined with compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d3e9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressing_caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                                      port=6379,\n",
    "                                                      database=0,\n",
    "                                                      prefix=\"json+compression\",\n",
    "                                                      serder=\"JSON\",\n",
    "                                                      use_compression=True)\n",
    "\n",
    "compressing_decorated_model = compressing_caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3125f1",
   "metadata": {},
   "source": [
    "The first time we make a prediction, the model will be used and the prediction will get serialized to JSON, then compressed, and saved to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cbf88b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15207.01)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=53, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=25.0,\n",
    "    children=4,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = compressing_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fe967",
   "metadata": {},
   "source": [
    "The second time we make a prediction, the compressed cache entry will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c0c2b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=15207.01)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=53, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=25.0,\n",
    "    children=4,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = compressing_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654692c",
   "metadata": {},
   "source": [
    "The compression works, now we'll do some testing to see if it improves the serialization/deserialization performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3def91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_total_seconds = timeit.timeit(\"[compressing_decorated_model.predict(sample) for sample in samples]\", number=1, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e7e9431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorator that does compressiontook around 35.559 ms to make a prediction and add it to the cache the first time that it sees the prediction inputs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_seconds_per_sample = compressed_total_seconds / 1000.0\n",
    "compressed_milliseconds_per_sample = compressed_seconds_per_sample * 1000.0\n",
    "\n",
    "md(\"The decorator that does compressiontook around {} ms to make a prediction and add it to the cache the \"\n",
    "   \"first time that it sees the prediction inputs.\".\n",
    "   format(round(compressed_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0b98c",
   "metadata": {},
   "source": [
    "Most of the time for this step is taken up by the model's prediction algorithm.\n",
    "\n",
    "Now we can try the same predictions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1504d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_total_seconds = timeit.timeit(\"[compressing_decorated_model.predict(sample) for sample in samples]\", number=1, globals=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "277a16d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The decorator that uses compressed JSON took 0.853 ms the second time that it saw the prediction inputs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_seconds_per_sample = compressed_total_seconds / 1000.0\n",
    "compressed_milliseconds_per_sample = compressed_seconds_per_sample * 1000.0\n",
    "\n",
    "md(\"The decorator that uses compressed JSON took {} ms the second time that it saw the prediction inputs.\".\n",
    "   format(round(compressed_milliseconds_per_sample, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68c1977c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The serder that uses JSON serialization and compression performs slightly better than the JSON serder. The test we did with uncompressed JSON above took about 0.8935623240000012 ms for each sample."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"The serder that uses JSON serialization and compression performs slightly better than the JSON serder. \"\n",
    "   \"The test we did with uncompressed JSON above took about {} ms for each sample.\".\n",
    "   format(decorated_milliseconds_per_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8043f1",
   "metadata": {},
   "source": [
    "We can see how much space the cache entries is taking up by querying each key and summing up the number of bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be3093fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The keys in the original JSON cache took up a total of 20630 bytes. The keys in the MessagePack cache take up a total of 18018 bytes. The keys in the compressed JSON cache take up a total of 22633 bytes, and the average number of bytes per cache entry is 22.61."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_number_of_bytes = 0\n",
    "compressed_total_entries = 0\n",
    "for key in r.scan_iter(\"json+compression*\"):\n",
    "    compressed_number_of_bytes += len(r.get(key))\n",
    "    compressed_total_entries = compressed_total_entries + 1\n",
    "    \n",
    "compressed_average_number_of_bytes = compressed_number_of_bytes / compressed_total_entries\n",
    "\n",
    "md(\"The keys in the original JSON cache took up a total of {} bytes. \"\n",
    "   \"The keys in the MessagePack cache take up a total of {} bytes. \"\n",
    "   \"The keys in the compressed JSON cache take up a total of {} bytes, \"\n",
    "   \"and the average number of bytes per cache entry is {}.\".\n",
    "   format(decorated_number_of_bytes, \n",
    "          msgpack_number_of_bytes,\n",
    "          compressed_number_of_bytes,\n",
    "          round(compressed_average_number_of_bytes, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0e8f2",
   "metadata": {},
   "source": [
    "The keys that were serialized with JSON and compressed were a few bytes bigger than the keys serialized and not compressed. It seems that compression is not saving memory in the cache, this is probably due to the small size of the entries and the fact that information was not repeated inside of the serialized data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8182e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492548d",
   "metadata": {},
   "source": [
    "### Reducing the Precision of the Inputs\n",
    "\n",
    "We can also try to limit the size of the cache by reducing the number of possible inputs to the hashing function. We'll demonstrate this with a few examples.\n",
    "\n",
    "We'll start by hashing a single sample of the input of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8dc6e661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4604654438722747517"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12345,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe843a7",
   "metadata": {},
   "source": [
    "Next, we'll hash a very similar model input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5b01610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3599773909132942364"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12346,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "model_input_dict = model_input.dict()\n",
    "frozen_dict = frozenset(model_input_dict.keys()), frozenset(model_input_dict.values())\n",
    "hash(frozen_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21120ce",
   "metadata": {},
   "source": [
    "The hash value produced is the second time is completely different even though the \"bmi\" field only changed by 0.00001. This means that these two predictions will have two different cache entries even though they are very lilely to be exactly the same prediction. Just to make sure, we'll make the predictions using these inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bba9906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12345,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdc911",
   "metadata": {},
   "source": [
    "Let's try the prediction and hash with a different value for the \"bmi\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85d460e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12346,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2c797",
   "metadata": {},
   "source": [
    "The prediction came out to be the same for both values of \"bmi\". However, the hashed value of the input was completely different. These predictions would be saved separately from each other in the cache, event though they are exactly the same. We can cut down on the number of entries in the cache by reducing the precision of floating point numbers so that these predictions can be cached one time instead of many. By rounding down the number we'll be reducing the number of cache entries that will be placed in the cache but also affecting the accuracy of the model's predictions. \n",
    "\n",
    "The caching decorator supports this feature, we'll just enable it by adding the \"reduced_precision_fields\" and \"number_of_places\" options to the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65e965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_precision_caching_decorator = RedisCachingDecorator(host=\"localhost\", \n",
    "                                                        port=6379,\n",
    "                                                        database=0,\n",
    "                                                        prefix=\"power_precision\",\n",
    "                                                        reduced_precision_fields=[\"bmi\"],\n",
    "                                                        number_of_places=0)\n",
    "\n",
    "low_precision_decorated_model = low_precision_caching_decorator.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625ccd9",
   "metadata": {},
   "source": [
    "The first time we make a prediction, the model will be used and the prediction input will get the precision of the \"bmi\" field reduced to one decimal place, then the prediction will get serialized to JSON, and saved to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d13d1625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.12345,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = low_precision_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e507afa",
   "metadata": {},
   "source": [
    "The second time the prediction is requested, the precision of the \"bmi\" field is reduced again in the same way, making the prediction input the same as before even though the values are not exactly the same. This will create the same hashed value which will retrieve the prediction from the cache and return it to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be4d0f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsuranceChargesModelOutput(charges=8640.78)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = InsuranceChargesModelInput(\n",
    "    age=42, \n",
    "    sex=SexEnum.female,\n",
    "    bmi=24.4321,\n",
    "    children=2,\n",
    "    smoker=False,\n",
    "    region=RegionEnum.northwest)\n",
    "\n",
    "prediction = low_precision_decorated_model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd6560",
   "metadata": {},
   "source": [
    "The predictions are the same even though the inputs were different. \n",
    "\n",
    "We can check on the performance by making 10000 predictions with the decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f7e0e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6cbca9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    low_precision_decorated_model.predict(generate_record())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620365c0",
   "metadata": {},
   "source": [
    "We can expect to find around 10,000 predictions in the cache.\n",
    "\n",
    "We can view the number of entries in the cache with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89039cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9430\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli DBSIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99d4b8",
   "metadata": {},
   "source": [
    "The fact that we reduced the precision of the \"bmi\" field cause the decorator to save around 500 fewer predictions in the cache.\n",
    "\n",
    "Although this is not always an ideal way to save memory, there are some model deployments that can benefit from this approach. All that is needed is to analyze how much precision the model needs from its numerical inputs. It rarely makes sense to store predictions with an unlimited precision in their inputs in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7e1c0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec local-redis redis-cli FLUSHDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ad11e",
   "metadata": {},
   "source": [
    "Now that we're done with the local redis instance we'll stop the docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39019a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-redis\n",
      "local-redis\n"
     ]
    }
   ],
   "source": [
    "!docker kill local-redis\n",
    "!docker rm local-redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1a57a",
   "metadata": {},
   "source": [
    "## Using a Cache Decorator in Production\n",
    "\n",
    "Adding a highly available strategy …\n",
    "\n",
    "What is the cache eviction policy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661cd0cb",
   "metadata": {},
   "source": [
    "## Creating the Model Service\n",
    "\n",
    "In order to deploy the Redis cache decorator in a real-world scenario, we’ll deploy it along with the model inside of a RESTful model service. In order to do this we’ll use the rest_model_service package that we developed in this blog post.\n",
    "\n",
    "## Creating a Docker Image\n",
    "\n",
    "## Deploying the Solution\n",
    "To show the system in action, we’ll deploy the service and the Redis instance to a Kubernetes cluster. \n",
    "\n",
    "## Create a deployment and service for Redis …\n",
    "\n",
    "## Create a configuration for REST model service …\n",
    "\n",
    "## Create a docker image for the service, model, and decorator…\n",
    "\n",
    "## Create a service for REST model service …\n",
    "\n",
    "## Create a deployment for REST model service … \n",
    "\n",
    "## Closing\n",
    "\n",
    "In this blog post, we showed how to build a decorator class that is able to cache predictions made by a machine learning model. Caching is a simple way to speed up predictions that we know can be reused and are requested often from a model. \n",
    "\n",
    "The cache decorator classes can be applied to any model that uses the MLModel base class without having to modify the model class at all. The caching functionality is contained completely in the RedisCacheDecorator class. The same thing is true for the RESTful model service, the cache functionality did not need to be added to the service because we separated the concerns of the service and the cache decorator. We were able to add caching to the deployed model by modifying the configuration. \n",
    "\n",
    "By using decorators we’re able to create software components that can be reused in many different contexts. For example, if we chose to deploy the cache decorator in gRPC service we should be able to do so as long as we instantiate and manage the decorator instance correctly.\n",
    "\n",
    "In the implementation of the decorators that we presented in this blog post each prediction is identified by hashing all of the input fields of the prediction request. This might not be necessary or advisable in different scenarios. In order to support different scenarios, we can provide a way to select the fields that are hashed together to create the identifier for the prediction. In this way, certain inputs can be ignored when selecting a prediction that is saved in the cache. Another way of doing this is to use a specific identifier field that is provided by the client in order to identify the prediction, however this puts the responsibility for determining how caching is done on the client that is using the predictions.\n",
    "\n",
    "Caching is often done to speed up operations that rely on I/O heavily, in the model deployment that we did in this blog post the model did not rely on I/O to make a prediction so it did not benefit from caching as much. An example of model deployments that rely on I/O is a model that needs to do data enrichment in order to make a prediction. Although most caching is done to speed up operations that rely on I/O, there are some types of models that are CPU intensive, such models would also benefit for \n",
    "\n",
    "Combining the caching decorator with other decorators that require I/O like data enrichment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf160ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a734c437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd57d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20fead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f94a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f270f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec47608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05c5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975fe3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad97f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97c4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28984acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35198b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c57543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e84016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
